{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5aad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import youtube_transcript_api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d478dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Hugging Face model and tokenizer for embeddings\n",
    "def initialize_embedding_model():\n",
    "    embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "    model = AutoModel.from_pretrained(embedding_model)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ec41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate embeddings for text\n",
    "def get_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d390ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocessing - Clean and normalize text\n",
    "def preprocess_text(text, preserve_code=True):\n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove special characters, keep alphanumeric and basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?-]', ' ', text)\n",
    "    \n",
    "    # Replace multiple spaces, newlines, or tabs with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Convert to lowercase (except for code snippets if preserve_code is True)\n",
    "    if not preserve_code:\n",
    "        text = text.lower()\n",
    "    else:\n",
    "        # Preserve code snippets (assuming they are within triple backticks or indentation)\n",
    "        code_blocks = []\n",
    "        def code_replacement(match):\n",
    "            code_blocks.append(match.group(0))\n",
    "            return f\"__CODE_BLOCK_{len(code_blocks)-1}__\"\n",
    "        \n",
    "        text = re.sub(r'```[\\s\\S]*?```', code_replacement, text)\n",
    "        text = re.sub(r'^\\s{4,}.*$', code_replacement, text, flags=re.MULTILINE)\n",
    "        text = text.lower()\n",
    "        # Restore code blocks\n",
    "        for i, code in enumerate(code_blocks):\n",
    "            text = text.replace(f\"__CODE_BLOCK_{i}__\", code)\n",
    "    \n",
    "    # Optionally remove stopwords (disabled by default for programming context)\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # words = text.split()\n",
    "    # text = ' '.join(word for word in words if word.lower() not in stop_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c670f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract text from PDF with structuring and page range support\n",
    "def extract_text_from_pdf(pdf_path, start_page=None, end_page=None):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text_chunks = []\n",
    "            current_section = \"\"\n",
    "            current_text = \"\"\n",
    "            \n",
    "            # Set default page range if not specified\n",
    "            if start_page is None:\n",
    "                start_page = 0\n",
    "            else:\n",
    "                start_page = max(0, start_page - 1)  # convert to 1-based to 0-based index\n",
    "            \n",
    "            if end_page is None:\n",
    "                end_page = len(reader.pages)\n",
    "            else:\n",
    "                end_page = min(end_page, len(reader.pages))\n",
    "            \n",
    "            # Validate page range\n",
    "            if start_page >= end_page:\n",
    "                print(f\"Invalid page range: start_page {start_page+1} >= end_page {end_page}\")\n",
    "                return []\n",
    "            \n",
    "            # Process only the specified page range\n",
    "            for page_num in range(start_page, end_page):\n",
    "                page = reader.pages[page_num]\n",
    "                text = page.extract_text() or \"\"\n",
    "                lines = text.split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    # Heuristic: Assume lines with all caps or short length are headings\n",
    "                    if line.isupper() or (len(line.strip()) < 50 and line.strip().endswith(':')):\n",
    "                        if current_text:\n",
    "                            text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "                            current_text = \"\"\n",
    "                        current_section = line.strip()\n",
    "                    else:\n",
    "                        current_text += \" \" + line\n",
    "                \n",
    "                if current_text:\n",
    "                    text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "                    current_text = \"\"\n",
    "            \n",
    "            return text_chunks  # List of (section, text) tuples\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6197825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Extract text from PDF with structuring\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     try:\n",
    "#         with open(pdf_path, 'rb') as file:\n",
    "#             reader = PyPDF2.PdfReader(file)\n",
    "#             text_chunks = []\n",
    "#             current_section = \"\"\n",
    "#             current_text = \"\"\n",
    "            \n",
    "#             for page in reader.pages:\n",
    "#                 text = page.extract_text() or \"\"\n",
    "#                 lines = text.split('\\n')\n",
    "#                 for line in lines:\n",
    "#                     # Heuristic: Assume lines with all caps or short length are headings\n",
    "#                     if line.isupper() or (len(line.strip()) < 50 and line.strip().endswith(':')):\n",
    "#                         if current_text:\n",
    "#                             text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "#                             current_text = \"\"\n",
    "#                         current_section = line.strip()\n",
    "#                     else:\n",
    "#                         current_text += \" \" + line\n",
    "#                 if current_text:\n",
    "#                     text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "#                     current_text = \"\"\n",
    "            \n",
    "#             return text_chunks  # List of (section, text) tuples\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc24dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract text from website with structuring\n",
    "def extract_text_from_website(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove script, style, and navigation elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        text_chunks = []\n",
    "        current_section = \"\"\n",
    "        current_text = \"\"\n",
    "        \n",
    "        # Extract headings (h1, h2, h3) and their content\n",
    "        for element in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                if current_text:\n",
    "                    text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "                    current_text = \"\"\n",
    "                current_section = element.get_text(strip=True)\n",
    "            else:\n",
    "                current_text += \" \" + element.get_text(strip=True)\n",
    "        \n",
    "        if current_text:\n",
    "            text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "        \n",
    "        return text_chunks  # List of (section, text) tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from website {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679baaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Extract text from YouTube video transcript with structuring\n",
    "def extract_youtube_transcript(video_url):\n",
    "    try:\n",
    "        video_id = None\n",
    "        if \"youtube.com\" in video_url or \"youtu.be\" in video_url:\n",
    "            match = re.search(r\"(?:v=|youtu\\.be/)([0-9A-Za-z_-]{11})\", video_url)\n",
    "            if match:\n",
    "                video_id = match.group(1)\n",
    "        \n",
    "        if not video_id:\n",
    "            print(f\"Invalid YouTube URL: {video_url}\")\n",
    "            return []\n",
    "            \n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        text_chunks = []\n",
    "        current_text = \"\"\n",
    "        current_timestamp = 0\n",
    "        \n",
    "        # Group transcript by time intervals (e.g., every 60 seconds)\n",
    "        for entry in transcript:\n",
    "            if entry['start'] > current_timestamp + 60:\n",
    "                if current_text:\n",
    "                    text_chunks.append((f\"Segment_{int(current_timestamp)}\", preprocess_text(current_text)))\n",
    "                    current_text = \"\"\n",
    "                current_timestamp = entry['start']\n",
    "            current_text += \" \" + entry['text']\n",
    "        \n",
    "        if current_text:\n",
    "            text_chunks.append((f\"Segment_{int(current_timestamp)}\", preprocess_text(current_text)))\n",
    "        \n",
    "        return text_chunks  # List of (segment, text) tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting transcript from YouTube video {video_url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73ab253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Cache preprocessed text to disk\n",
    "def cache_text(source, text_chunks, cache_dir=\"../cache\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    source_name = re.sub(r'[^\\w\\-_\\.]', '_', source)\n",
    "    cache_path = os.path.join(cache_dir, f\"{source_name}.txt\")\n",
    "    \n",
    "    try:\n",
    "        with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "            for section, text in text_chunks:\n",
    "                f.write(f\"--- {section} ---\\n{text}\\n\\n\")\n",
    "        print(f\"Cached text for {source} to {cache_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error caching text for {source}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90949e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Check cache for preprocessed text\n",
    "def load_from_cache(source, cache_dir=\"../cache\"):\n",
    "    source_name = re.sub(r'[^\\w\\-_\\.]', '_', source)\n",
    "    cache_path = os.path.join(cache_dir, f\"{source_name}.txt\")\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                chunks = []\n",
    "                current_section = \"\"\n",
    "                current_text = \"\"\n",
    "                for line in content.splitlines():\n",
    "                    if line.startswith(\"--- \") and line.endswith(\" ---\"):\n",
    "                        if current_text:\n",
    "                            chunks.append((current_section, current_text.strip()))\n",
    "                            current_text = \"\"\n",
    "                        current_section = line[4:-4].strip()\n",
    "                    else:\n",
    "                        current_text += \" \" + line\n",
    "                if current_text:\n",
    "                    chunks.append((current_section, current_text.strip()))\n",
    "                return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cache for {source}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adb05a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Initialize embedding model and generate sample embeddings for review\n",
    "def generate_sample_embeddings(text_chunks, sources, num_samples=5):\n",
    "    \"\"\"\n",
    "    Initialize embedding model and generate sample embeddings for review.\n",
    "    Does not store embeddings, only displays them for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        text_chunks: List of text chunks from different sources\n",
    "        sources: List of source names corresponding to text_chunks\n",
    "        num_samples: Number of sample embeddings to generate and display\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the embedding model\n",
    "    print(\"Initializing embedding model...\")\n",
    "    tokenizer, model = initialize_embedding_model()\n",
    "    print(\"Embedding model initialized successfully!\\n\")\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    for source, chunks in zip(sources, text_chunks):\n",
    "        if not chunks or sample_count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing source: {source}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (section, text) in enumerate(chunks):\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            # Split text into smaller chunks (~500 words) for embedding\n",
    "            words = text.split()\n",
    "            chunk_size = 500\n",
    "            sub_chunks = [\" \".join(words[j:j + chunk_size]) for j in range(0, len(words), chunk_size)]\n",
    "            \n",
    "            for j, sub_chunk in enumerate(sub_chunks):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                # Generate embedding for the sub-chunk\n",
    "                print(f\"\\nSample {sample_count + 1}:\")\n",
    "                print(f\"Source: {source}\")\n",
    "                print(f\"Section: {section}\")\n",
    "                print(f\"Chunk ID: {i}_{j}\")\n",
    "                print(f\"Text preview (first 100 chars): {sub_chunk[:100]}...\")\n",
    "                \n",
    "                # Get the embedding\n",
    "                embedding = get_embeddings(sub_chunk, tokenizer, model)\n",
    "                \n",
    "                # Display embedding information\n",
    "                print(f\"Embedding shape: {embedding.shape}\")\n",
    "                print(f\"Embedding type: {type(embedding)}\")\n",
    "                print(f\"First 10 dimensions: {embedding[:10].tolist()}\")\n",
    "                # print(f\"Embedding norm: {float(embedding.norm()):.4f}\")\n",
    "                print(f\"Min value: {float(embedding.min()):.4f}\")\n",
    "                print(f\"Max value: {float(embedding.max()):.4f}\")\n",
    "                print(f\"Mean value: {float(embedding.mean()):.4f}\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                sample_count += 1\n",
    "    \n",
    "    print(f\"\\nGenerated and displayed {sample_count} sample embeddings for review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02837329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Initialize ChromaDB with persistence and store embeddings \n",
    "def store_in_chromadb(text_chunks, sources, collection_name=\"python_tutor\", persist_directory=\"../data/chroma_db\"):\n",
    "    # Initialize ChromaDB client with persistent storage\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=persist_directory\n",
    "    )\n",
    "    try:\n",
    "        # Try to get existing collection or create new one\n",
    "        collection = client.get_or_create_collection(name=collection_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing/creating collection: {e}\")\n",
    "        return\n",
    "\n",
    "    tokenizer, model = initialize_embedding_model()\n",
    "    \n",
    "    for source, chunks in zip(sources, text_chunks):\n",
    "        if not chunks:\n",
    "            continue\n",
    "            \n",
    "        for i, (section, text) in enumerate(chunks):\n",
    "            # Split text into smaller chunks (~500 words) for embedding\n",
    "            words = text.split()\n",
    "            chunk_size = 500\n",
    "            sub_chunks = [\" \".join(words[j:j + chunk_size]) for j in range(0, len(words), chunk_size)]\n",
    "            \n",
    "            for j, sub_chunk in enumerate(sub_chunks):\n",
    "                embedding = get_embeddings(sub_chunk, tokenizer, model)\n",
    "                collection.add(\n",
    "                    documents=[sub_chunk],\n",
    "                    embeddings=[embedding.tolist()],\n",
    "                    metadatas=[{\"source\": source, \"section\": section, \"chunk_id\": f\"{i}_{j}\"}],\n",
    "                    ids=[f\"{source}_{i}_{j}\"]\n",
    "                )\n",
    "                print(f\"Stored chunk {i}_{j} from {source} (section: {section})\")\n",
    "    \n",
    "    print(f\"Database persisted to {persist_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a68b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process all sources\n",
    "def process_knowledge_base(pdf_paths, website_urls, youtube_urls, cache_dir=\"..\\cache\", \n",
    "                          pdf_start_page=None, pdf_end_page=None):\n",
    "    texts = []\n",
    "    sources = []\n",
    "    \n",
    "    # # Process PDFs\n",
    "    # for pdf_path in pdf_paths:\n",
    "    #     cached = load_from_cache(pdf_path, cache_dir)\n",
    "    #     if cached:\n",
    "    #         texts.append(cached)\n",
    "    #         sources.append(pdf_path)\n",
    "    #     else:\n",
    "    #         chunks = extract_text_from_pdf(pdf_path, start_page=pdf_start_page, end_page=pdf_end_page)\n",
    "    #         if chunks:\n",
    "    #             cache_text(pdf_path, chunks, cache_dir)\n",
    "    #             texts.append(chunks)\n",
    "    #             sources.append(pdf_path)\n",
    "    \n",
    "    # Process websites\n",
    "    for url in website_urls:\n",
    "        cached = load_from_cache(url, cache_dir)\n",
    "        if cached:\n",
    "            texts.append(cached)\n",
    "            sources.append(url)\n",
    "        else:\n",
    "            chunks = extract_text_from_website(url)\n",
    "            if chunks:\n",
    "                cache_text(url, chunks, cache_dir)\n",
    "                texts.append(chunks)\n",
    "                sources.append(url)\n",
    "    \n",
    "    # # Process YouTube videos\n",
    "    # for video_url in youtube_urls:\n",
    "    #     cached = load_from_cache(video_url, cache_dir)\n",
    "    #     if cached:\n",
    "    #         texts.append(cached)\n",
    "    #         sources.append(video_url)\n",
    "    #     else:\n",
    "    #         chunks = extract_youtube_transcript(video_url)\n",
    "    #         if chunks:\n",
    "    #             cache_text(video_url, chunks, cache_dir)\n",
    "    #             texts.append(chunks)\n",
    "    #             sources.append(video_url)\n",
    "    \n",
    "    # generate_sample_embeddings for review\n",
    "    generate_sample_embeddings(texts, sources)\n",
    "\n",
    "    store_in_chromadb(texts, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main function to process all sources\n",
    "# def process_knowledge_base(pdf_paths, website_urls, youtube_urls, cache_dir=\"..\\cache\"):\n",
    "#     texts = []\n",
    "#     sources = []\n",
    "    \n",
    "#     # Process PDFs\n",
    "#     for pdf_path in pdf_paths:\n",
    "#         cached = load_from_cache(pdf_path, cache_dir)\n",
    "#         if cached:\n",
    "#             texts.append(cached)\n",
    "#             sources.append(pdf_path)\n",
    "#         else:\n",
    "#             chunks = extract_text_from_pdf(pdf_path)\n",
    "#             if chunks:\n",
    "#                 cache_text(pdf_path, chunks, cache_dir)\n",
    "#                 texts.append(chunks)\n",
    "#                 sources.append(pdf_path)\n",
    "    \n",
    "#     # # Process websites\n",
    "#     # for url in website_urls:\n",
    "#     #     cached = load_from_cache(url, cache_dir)\n",
    "#     #     if cached:\n",
    "#     #         texts.append(cached)\n",
    "#     #         sources.append(url)\n",
    "#     #     else:\n",
    "#     #         chunks = extract_text_from_website(url)\n",
    "#     #         if chunks:\n",
    "#     #             cache_text(url, chunks, cache_dir)\n",
    "#     #             texts.append(chunks)\n",
    "#     #             sources.append(url)\n",
    "    \n",
    "#     # # Process YouTube videos\n",
    "#     # for video_url in youtube_urls:\n",
    "#     #     cached = load_from_cache(video_url, cache_dir)\n",
    "#     #     if cached:\n",
    "#     #         texts.append(cached)\n",
    "#     #         sources.append(video_url)\n",
    "#     #     else:\n",
    "#     #         chunks = extract_youtube_transcript(video_url)\n",
    "#     #         if chunks:\n",
    "#     #             cache_text(video_url, chunks, cache_dir)\n",
    "#     #             texts.append(chunks)\n",
    "#     #             sources.append(video_url)\n",
    "    \n",
    "#     # generate_sample_embeddings for review\n",
    "#     generate_sample_embeddings(texts, sources)\n",
    "\n",
    "#     store_in_chromadb(texts, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b394407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached text for https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ to ..\\cache\\https___www.geeksforgeeks.org_how-to-learn-python-from-scratch_.txt\n",
      "Initializing embedding model...\n",
      "Embedding model initialized successfully!\n",
      "\n",
      "Processing source: https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "Source: https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\n",
      "Section: \n",
      "Chunk ID: 0_0\n",
      "Text preview (first 100 chars): python tutorial interview questions python quiz python glossary python projects practice python data...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [-0.4097895622253418, 0.07391665875911713, 0.08384502679109573, 0.19434501230716705, -0.2590826749801636, -0.6343346834182739, 0.15607039630413055, -0.0026295939460396767, -0.40203383564949036, -0.06282304972410202]\n",
      "Min value: -0.6343\n",
      "Max value: 0.6420\n",
      "Mean value: -0.0036\n",
      "============================================================\n",
      "\n",
      "Sample 2:\n",
      "Source: https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\n",
      "Section: How to Learn Python from Scratch in 2025\n",
      "Chunk ID: 1_0\n",
      "Text preview (first 100 chars): python is a general-purpose high-level programming language and is widely used among the developers ...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [-0.08734782785177231, -0.02162032574415207, -0.04825260117650032, 0.046103302389383316, -0.0945175513625145, -0.27222496271133423, -0.006758498027920723, 0.13858678936958313, -0.2094181925058365, -0.03642220422625542]\n",
      "Min value: -0.3584\n",
      "Max value: 0.4348\n",
      "Mean value: -0.0019\n",
      "============================================================\n",
      "\n",
      "Sample 3:\n",
      "Source: https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\n",
      "Section: Key features of Python\n",
      "Chunk ID: 2_0\n",
      "Text preview (first 100 chars): pythonhas many reasons for being popular and in demand. a few of the reasons are mentioned below. em...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [-0.15769848227500916, -0.06702055037021637, 0.06121724471449852, 0.06953011453151703, -0.029708169400691986, -0.16886620223522186, -0.03683580458164215, 0.2520602345466614, 0.08102218061685562, 0.06011536717414856]\n",
      "Min value: -0.4336\n",
      "Max value: 0.3743\n",
      "Mean value: -0.0031\n",
      "============================================================\n",
      "\n",
      "Sample 4:\n",
      "Source: https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\n",
      "Section: Getting started with Python Programming –\n",
      "Chunk ID: 4_0\n",
      "Text preview (first 100 chars): python is a lot easier to code and learn. python programs can be written on any plain text editor li...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [-0.13691073656082153, -0.014268707484006882, 0.019058832898736, 0.12888337671756744, -0.019951509311795235, -0.24486297369003296, -0.12068398296833038, 0.1877061277627945, 0.007368766237050295, 0.12185857445001602]\n",
      "Min value: -0.3424\n",
      "Max value: 0.4056\n",
      "Mean value: -0.0024\n",
      "============================================================\n",
      "\n",
      "Sample 5:\n",
      "Source: https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\n",
      "Section: What if Python already exists? Let’s check\n",
      "Chunk ID: 5_0\n",
      "Text preview (first 100 chars): windows dont come with python preinstalled, it needs to be installed explicitly. but unlike windows,...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [0.03187127411365509, -0.04494912922382355, 0.12006057053804398, 0.031460173428058624, 0.11046262085437775, -0.05758148059248924, -0.02177647314965725, -0.0371355302631855, -0.03310763090848923, 0.01722218655049801]\n",
      "Min value: -0.3379\n",
      "Max value: 0.2624\n",
      "Mean value: -0.0024\n",
      "============================================================\n",
      "\n",
      "Generated and displayed 5 sample embeddings for review.\n",
      "Stored chunk 0_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: )\n",
      "Stored chunk 1_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: How to Learn Python from Scratch in 2025)\n",
      "Stored chunk 2_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Key features of Python)\n",
      "Stored chunk 4_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Getting started with Python Programming –)\n",
      "Stored chunk 5_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: What if Python already exists? Let’s check)\n",
      "Stored chunk 6_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Download and Installation)\n",
      "Stored chunk 7_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: How to run a Python program)\n",
      "Stored chunk 8_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Windows)\n",
      "Stored chunk 9_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Unix/Linux)\n",
      "Stored chunk 10_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Python Indentation)\n",
      "Stored chunk 11_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Python Comments)\n",
      "Stored chunk 12_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Variables)\n",
      "Stored chunk 13_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Operators)\n",
      "Stored chunk 14_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Taking input from user –)\n",
      "Stored chunk 15_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Printing output to console –)\n",
      "Stored chunk 16_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Data Types)\n",
      "Stored chunk 17_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Numeric)\n",
      "Stored chunk 18_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Sequence Type)\n",
      "Stored chunk 19_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Boolean)\n",
      "Stored chunk 20_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Set)\n",
      "Stored chunk 21_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Dictionary)\n",
      "Stored chunk 22_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Decision Making)\n",
      "Stored chunk 23_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 23_1 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 23_2 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 23_3 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 23_4 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 23_5 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 23_6 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 23_7 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Control flow (Loops))\n",
      "Stored chunk 24_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Loop control statements)\n",
      "Stored chunk 25_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Functions)\n",
      "Stored chunk 26_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Function with arguments)\n",
      "Stored chunk 27_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Lambda functions)\n",
      "Stored chunk 28_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Object Oriented Programming)\n",
      "Stored chunk 29_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Classes and Objects)\n",
      "Stored chunk 30_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Constructors and Destructors)\n",
      "Stored chunk 31_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Inheritance)\n",
      "Stored chunk 32_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Encapsulation)\n",
      "Stored chunk 33_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Polymorphism)\n",
      "Stored chunk 34_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: File Handling)\n",
      "Stored chunk 35_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Modules)\n",
      "Stored chunk 36_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Packages)\n",
      "Stored chunk 37_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Regular expressions(RegEx))\n",
      "Stored chunk 38_0 from https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/ (section: Exception handling)\n",
      "Database persisted to ../data/chroma_db\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_paths = [\n",
    "        \"../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\"\n",
    "    ]\n",
    "    website_urls = [\n",
    "        \"https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\"\n",
    "    ]\n",
    "    youtube_urls = [\n",
    "        \"https://www.youtube.com/watch?v=8124kv-632k\"\n",
    "    ]\n",
    "    \n",
    "    process_knowledge_base(pdf_paths, website_urls, youtube_urls, \n",
    "                          pdf_start_page=42, pdf_end_page=703)  # Example page range for PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9651ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"api key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca40197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms import LLMMetadata, LLM\n",
    "# from llama_index import Settings\n",
    "\n",
    "Settings.llm = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcb8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import all required libraries\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import PromptTemplate\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b24db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAG-based LLM Tutor Implementation...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Complete RAG-based LLM Tutor Implementation\n",
    "# Steps 11-15: From Vector Database to Interactive Gradio Interface\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging to reduce noise\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "print(\"Starting RAG-based LLM Tutor Implementation...\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64079a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 11: Set up the RAG Framework\n",
    "# =============================================================================\n",
    "\n",
    "class RAGTutor:\n",
    "    def __init__(self, persist_directory=\"../data/chroma_db\", collection_name=\"python_tutor\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG Tutor with ChromaDB connection and embedding model\n",
    "        \"\"\"\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.embedding_model = None\n",
    "        self.tokenizer = None\n",
    "        self.llm_pipeline = None\n",
    "        self.query_engine = None\n",
    "        \n",
    "        print(\"Step 11: Setting up RAG Framework...\")\n",
    "        self._setup_embedding_model()\n",
    "        self._connect_to_chromadb()\n",
    "        self._setup_llm_pipeline()\n",
    "        print(\"✓ RAG Framework setup complete!\")\n",
    "        \n",
    "    def _setup_embedding_model(self):\n",
    "        \"\"\"\n",
    "        Initialize the embedding model for text vectorization\n",
    "        \"\"\"\n",
    "        print(\"  - Initializing embedding model...\")\n",
    "        \n",
    "        # Use the same embedding model as in original code\n",
    "        embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        \n",
    "        # Initialize for direct use\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "        self.embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
    "        \n",
    "        # Initialize for LlamaIndex\n",
    "        self.embed_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
    "        \n",
    "        print(\"  ✓ Embedding model initialized\")\n",
    "        \n",
    "    def _connect_to_chromadb(self):\n",
    "        \"\"\"\n",
    "        Connect to existing ChromaDB collection\n",
    "        \"\"\"\n",
    "        print(\"  - Connecting to ChromaDB...\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to persistent ChromaDB\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get existing collection\n",
    "            self.collection = self.client.get_collection(name=self.collection_name)\n",
    "            \n",
    "            # Get collection info\n",
    "            collection_count = self.collection.count()\n",
    "            print(f\"  ✓ Connected to ChromaDB collection with {collection_count} documents\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error connecting to ChromaDB: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def _setup_llm_pipeline(self):\n",
    "        \"\"\"\n",
    "        Setup the language model pipeline for text generation\n",
    "        \"\"\"\n",
    "        print(\"  - Setting up LLM pipeline...\")\n",
    "        \n",
    "        # Use a smaller, efficient model for demonstration\n",
    "        # You can replace this with larger models like llama-2 if you have the resources\n",
    "        model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "        \n",
    "        try:\n",
    "            # Initialize the text generation pipeline\n",
    "            self.llm_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model_name,\n",
    "                tokenizer=model_name,\n",
    "                max_length=512,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=50256  # EOS token for GPT-based models\n",
    "            )\n",
    "            print(\"  ✓ LLM pipeline initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error setting up LLM: {e}\")\n",
    "            # Fallback to a simpler approach\n",
    "            print(\"  - Using fallback text generation approach...\")\n",
    "            self.llm_pipeline = None\n",
    "\n",
    "    def get_embeddings(self, text):\n",
    "        \"\"\"\n",
    "        Generate embeddings for given text (same as your original function)\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a63b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 12: Connect Retrieval and Generation using LlamaIndex\n",
    "# =============================================================================\n",
    "\n",
    "def setup_llama_index_rag(rag_tutor):\n",
    "    \"\"\"\n",
    "    Setup LlamaIndex RAG system with ChromaDB integration\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 12: Setting up LlamaIndex RAG Integration...\")\n",
    "    \n",
    "    try:\n",
    "        # Create ChromaVectorStore from existing collection\n",
    "        vector_store = ChromaVectorStore(chroma_collection=rag_tutor.collection)\n",
    "        \n",
    "        # Create storage context\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        # Configure LlamaIndex settings\n",
    "        Settings.embed_model = rag_tutor.embed_model\n",
    "        \n",
    "        # Create index from existing vector store\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "        \n",
    "        print(\"  ✓ LlamaIndex integration setup complete\")\n",
    "        return index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error setting up LlamaIndex: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_query_engine(index):\n",
    "    \"\"\"\n",
    "    Create a query engine for RAG retrieval and generation\n",
    "    \"\"\"\n",
    "    print(\"  - Creating query engine...\")\n",
    "    \n",
    "    # Define custom prompt template for Python tutoring\n",
    "    qa_prompt_template = PromptTemplate(\n",
    "        \"Context information is below.\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"{context_str}\\n\"\n",
    "        \"---------------------\\n\"\n",
    "        \"You are a helpful Python programming tutor. Use the context information to answer the question.\\n\"\n",
    "        \"If the context doesn't contain relevant information, say so and provide general guidance.\\n\"\n",
    "        \"Always explain concepts clearly and provide examples when helpful.\\n\"\n",
    "        \"Question: {query_str}\\n\"\n",
    "        \"Answer: \"\n",
    "    )\n",
    "    \n",
    "    # Create query engine\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=3,  # Retrieve top 3 most similar chunks\n",
    "        text_qa_template=qa_prompt_template\n",
    "    )\n",
    "    \n",
    "    print(\"  ✓ Query engine created\")\n",
    "    return query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97808635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 13: Add Basic Context Handling\n",
    "# =============================================================================\n",
    "\n",
    "class ConversationContext:\n",
    "    \"\"\"\n",
    "    Simple conversation context manager\n",
    "    \"\"\"\n",
    "    def __init__(self, max_history=5):\n",
    "        self.max_history = max_history\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def add_exchange(self, question, answer):\n",
    "        \"\"\"\n",
    "        Add a question-answer exchange to history\n",
    "        \"\"\"\n",
    "        self.conversation_history.append({\n",
    "            'question': question,\n",
    "            'answer': answer\n",
    "        })\n",
    "        \n",
    "        # Keep only recent history\n",
    "        if len(self.conversation_history) > self.max_history:\n",
    "            self.conversation_history.pop(0)\n",
    "            \n",
    "    def get_context_string(self):\n",
    "        \"\"\"\n",
    "        Get conversation history as context string\n",
    "        \"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"\"\n",
    "            \n",
    "        context_parts = []\n",
    "        for exchange in self.conversation_history[-3:]:  # Use last 3 exchanges\n",
    "            context_parts.append(f\"Previous Q: {exchange['question']}\")\n",
    "            context_parts.append(f\"Previous A: {exchange['answer'][:200]}...\")  # Truncate long answers\n",
    "            \n",
    "        return \"\\n\".join(context_parts)\n",
    "        \n",
    "    def clear_history(self):\n",
    "        \"\"\"\n",
    "        Clear conversation history\n",
    "        \"\"\"\n",
    "        self.conversation_history = []\n",
    "\n",
    "def enhanced_query_with_context(query_engine, question, context_manager):\n",
    "    \"\"\"\n",
    "    Enhanced query function with conversation context\n",
    "    \"\"\"\n",
    "    print(f\"\\nStep 13: Processing query with context...\")\n",
    "    \n",
    "    # Get conversation context\n",
    "    context = context_manager.get_context_string()\n",
    "    \n",
    "    # Enhance question with context if available\n",
    "    if context:\n",
    "        enhanced_question = f\"Context from previous conversation:\\n{context}\\n\\nCurrent question: {question}\"\n",
    "    else:\n",
    "        enhanced_question = question\n",
    "        \n",
    "    try:\n",
    "        # Query the RAG system\n",
    "        print(\"  - Retrieving relevant information...\")\n",
    "        response = query_engine.query(enhanced_question)\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = str(response)\n",
    "        \n",
    "        # Add to conversation history\n",
    "        context_manager.add_exchange(question, answer)\n",
    "        \n",
    "        print(\"  ✓ Query processed successfully\")\n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing query: {e}\")\n",
    "        fallback_answer = f\"I apologize, but I encountered an error processing your question about: {question}. Please try rephrasing your question or ask something else about Python programming.\"\n",
    "        context_manager.add_exchange(question, fallback_answer)\n",
    "        return fallback_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0691a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# STEP 14: Integrate RAG Backend with Gradio UI\n",
    "# =============================================================================\n",
    "\n",
    "def create_gradio_interface(rag_tutor, query_engine, context_manager):\n",
    "    \"\"\"\n",
    "    Create Gradio interface for the RAG tutor\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 14: Creating Gradio Interface...\")\n",
    "    \n",
    "    def chat_function(message, history):\n",
    "        \"\"\"\n",
    "        Main chat function for Gradio interface\n",
    "        \"\"\"\n",
    "        if not message.strip():\n",
    "            return \"Please ask a question about Python programming!\"\n",
    "            \n",
    "        # Process the query\n",
    "        response = enhanced_query_with_context(query_engine, message, context_manager)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def clear_conversation():\n",
    "        \"\"\"\n",
    "        Clear conversation history\n",
    "        \"\"\"\n",
    "        context_manager.clear_history()\n",
    "        return \"Conversation history cleared!\"\n",
    "        \n",
    "    def get_sample_questions():\n",
    "        \"\"\"\n",
    "        Provide sample questions for users\n",
    "        \"\"\"\n",
    "        samples = [\n",
    "            \"What are Python data types?\",\n",
    "            \"How do I create a for loop in Python?\",\n",
    "            \"What is the difference between lists and tuples?\",\n",
    "            \"How do I handle exceptions in Python?\",\n",
    "            \"What are Python functions and how do I create them?\"\n",
    "        ]\n",
    "        return \"\\n\".join([f\"• {q}\" for q in samples])\n",
    "    \n",
    "    # Create the Gradio interface\n",
    "    with gr.Blocks(title=\"Python RAG Tutor\", theme=gr.themes.Soft()) as interface:\n",
    "        gr.Markdown(\"# Python RAG Tutor\")\n",
    "        gr.Markdown(\"Ask me anything about Python programming! I'll search through my knowledge base to help you learn.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                # Chat interface\n",
    "                chatbot = gr.Chatbot(height=400, label=\"Python Tutor Chat\")\n",
    "                msg = gr.Textbox(\n",
    "                    label=\"Ask your Python question\",\n",
    "                    placeholder=\"Type your Python programming question here...\",\n",
    "                    lines=2\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    submit_btn = gr.Button(\"Ask Question\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "                    \n",
    "            with gr.Column(scale=1):\n",
    "                # Side panel with information\n",
    "                gr.Markdown(\"### Knowledge Base\")\n",
    "                gr.Markdown(\"I can help you with topics from:\")\n",
    "                gr.Markdown(\"• Python fundamentals\\n• Data structures\\n• Control flow\\n• Functions\\n• And more!\")\n",
    "                \n",
    "                gr.Markdown(\"### Sample Questions\")\n",
    "                sample_display = gr.Textbox(\n",
    "                    value=get_sample_questions(),\n",
    "                    label=\"Try asking:\",\n",
    "                    lines=8,\n",
    "                    interactive=False\n",
    "                )\n",
    "                \n",
    "                # Status information\n",
    "                gr.Markdown(\"### System Status\")\n",
    "                status_text = f\"Connected to knowledge base\\n {rag_tutor.collection.count()} documents loaded\"\n",
    "                gr.Textbox(value=status_text, label=\"Status\", lines=3, interactive=False)\n",
    "        \n",
    "        # Event handlers\n",
    "        def respond(message, chat_history):\n",
    "            if not message.strip():\n",
    "                return \"\", chat_history\n",
    "                \n",
    "            # Get bot response\n",
    "            bot_response = chat_function(message, chat_history)\n",
    "            \n",
    "            # Add to chat history\n",
    "            chat_history.append((message, bot_response))\n",
    "            \n",
    "            return \"\", chat_history\n",
    "        \n",
    "        def clear_chat():\n",
    "            clear_conversation()\n",
    "            return []\n",
    "            \n",
    "        # Connect events\n",
    "        submit_btn.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "        msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "        clear_btn.click(clear_chat, outputs=chatbot)\n",
    "    \n",
    "    print(\"  ✓ Gradio interface created\")\n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9fad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 15: Test the Prototype and Document Issues\n",
    "# =============================================================================\n",
    "\n",
    "def test_rag_system(rag_tutor, query_engine, context_manager):\n",
    "    \"\"\"\n",
    "    Test the RAG system with sample queries\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 15: Testing RAG System...\")\n",
    "    \n",
    "    test_questions = [\n",
    "        \"What are Python variables?\",\n",
    "        \"How do I create a list in Python?\",\n",
    "        \"What is a for loop?\",\n",
    "        \"Can you explain Python functions?\",\n",
    "        \"What are the main Python data types?\"\n",
    "    ]\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n--- Test {i}: {question} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Record start time\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Get response\n",
    "            response = enhanced_query_with_context(query_engine, question, context_manager)\n",
    "            \n",
    "            # Record end time\n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            # Store results\n",
    "            test_results.append({\n",
    "                'question': question,\n",
    "                'response': response[:200] + \"...\" if len(response) > 200 else response,\n",
    "                'response_time': response_time,\n",
    "                'success': True\n",
    "            })\n",
    "            \n",
    "            print(f\"Response time: {response_time:.2f} seconds\")\n",
    "            print(f\"Response preview: {response[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            test_results.append({\n",
    "                'question': question,\n",
    "                'response': f\"Error: {e}\",\n",
    "                'response_time': 0,\n",
    "                'success': False\n",
    "            })\n",
    "    \n",
    "    # Generate test report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    successful_tests = sum(1 for result in test_results if result['success'])\n",
    "    avg_response_time = sum(result['response_time'] for result in test_results if result['success'])\n",
    "    if successful_tests > 0:\n",
    "        avg_response_time /= successful_tests\n",
    "    \n",
    "    print(f\"Total tests: {len(test_questions)}\")\n",
    "    print(f\"Successful tests: {successful_tests}\")\n",
    "    print(f\"Failed tests: {len(test_questions) - successful_tests}\")\n",
    "    print(f\"Average response time: {avg_response_time:.2f} seconds\")\n",
    "    \n",
    "    # Document issues\n",
    "    print(\"\\n--- DOCUMENTED ISSUES ---\")\n",
    "    issues = []\n",
    "    \n",
    "    if successful_tests < len(test_questions):\n",
    "        issues.append(f\"• {len(test_questions) - successful_tests} queries failed\")\n",
    "    \n",
    "    if avg_response_time > 10:\n",
    "        issues.append(f\"• Slow response time: {avg_response_time:.2f}s average\")\n",
    "    \n",
    "    if len(issues) == 0:\n",
    "        print(\"✅ No major issues detected!\")\n",
    "    else:\n",
    "        for issue in issues:\n",
    "            print(issue)\n",
    "    \n",
    "    return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"api key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb7359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Complete RAG-based LLM Tutor...\n",
      "Step 11: Setting up RAG Framework...\n",
      "  - Initializing embedding model...\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "  ✓ Embedding model initialized\n",
      "  - Connecting to ChromaDB...\n",
      "  ✓ Connected to ChromaDB collection with 45 documents\n",
      "  - Setting up LLM pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43275d3ae874425aa8b3fc167e2ce733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ LLM pipeline initialized\n",
      "✓ RAG Framework setup complete!\n",
      "\n",
      "Step 12: Setting up LlamaIndex RAG Integration...\n",
      "  ✓ LlamaIndex integration setup complete\n",
      "  - Creating query engine...\n",
      "  ✓ Query engine created\n",
      "\n",
      "Step 13: Initializing conversation context...\n",
      "  ✓ Context manager initialized\n",
      "\n",
      "Step 15: Testing RAG System...\n",
      "\n",
      "--- Test 1: What are Python variables? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.397561 seconds\n",
      "Retrying request to /chat/completions in 0.397561 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.938094 seconds\n",
      "Retrying request to /chat/completions in 0.938094 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.552853 seconds\n",
      "Retrying request to /chat/completions in 1.552853 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.8670776628142635 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.8670776628142635 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.489086 seconds\n",
      "Retrying request to /chat/completions in 0.489086 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.776084 seconds\n",
      "Retrying request to /chat/completions in 0.776084 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.992230 seconds\n",
      "Retrying request to /chat/completions in 1.992230 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.776740185510109 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.776740185510109 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.442695 seconds\n",
      "Retrying request to /chat/completions in 0.442695 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.979148 seconds\n",
      "Retrying request to /chat/completions in 0.979148 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.924203 seconds\n",
      "Retrying request to /chat/completions in 1.924203 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 15.67 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: What are Python variables?. ...\n",
      "\n",
      "--- Test 2: How do I create a list in Python? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.478937 seconds\n",
      "Retrying request to /chat/completions in 0.478937 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.865756 seconds\n",
      "Retrying request to /chat/completions in 0.865756 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.972034 seconds\n",
      "Retrying request to /chat/completions in 1.972034 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.16148717721091677 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.16148717721091677 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.460024 seconds\n",
      "Retrying request to /chat/completions in 0.460024 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.862821 seconds\n",
      "Retrying request to /chat/completions in 0.862821 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.804078 seconds\n",
      "Retrying request to /chat/completions in 1.804078 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 1.1896546144782232 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.1896546144782232 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.407717 seconds\n",
      "Retrying request to /chat/completions in 0.407717 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.776714 seconds\n",
      "Retrying request to /chat/completions in 0.776714 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.875114 seconds\n",
      "Retrying request to /chat/completions in 1.875114 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 13.79 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: How do I create a list in Py...\n",
      "\n",
      "--- Test 3: What is a for loop? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.405303 seconds\n",
      "Retrying request to /chat/completions in 0.405303 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.998179 seconds\n",
      "Retrying request to /chat/completions in 0.998179 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.768130 seconds\n",
      "Retrying request to /chat/completions in 1.768130 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.713753027256101 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.713753027256101 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.418337 seconds\n",
      "Retrying request to /chat/completions in 0.418337 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.965639 seconds\n",
      "Retrying request to /chat/completions in 0.965639 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.570037 seconds\n",
      "Retrying request to /chat/completions in 1.570037 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.48900888346239735 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.48900888346239735 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.435052 seconds\n",
      "Retrying request to /chat/completions in 0.435052 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.900283 seconds\n",
      "Retrying request to /chat/completions in 0.900283 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.911588 seconds\n",
      "Retrying request to /chat/completions in 1.911588 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 11.97 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: What is a for loop?. Please ...\n",
      "\n",
      "--- Test 4: Can you explain Python functions? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.456792 seconds\n",
      "Retrying request to /chat/completions in 0.456792 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.841036 seconds\n",
      "Retrying request to /chat/completions in 0.841036 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.805324 seconds\n",
      "Retrying request to /chat/completions in 1.805324 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.033177423506039916 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.033177423506039916 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.436678 seconds\n",
      "Retrying request to /chat/completions in 0.436678 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.940214 seconds\n",
      "Retrying request to /chat/completions in 0.940214 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.603023 seconds\n",
      "Retrying request to /chat/completions in 1.603023 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5036019765340927 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5036019765340927 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.423044 seconds\n",
      "Retrying request to /chat/completions in 0.423044 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.832575 seconds\n",
      "Retrying request to /chat/completions in 0.832575 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.581027 seconds\n",
      "Retrying request to /chat/completions in 1.581027 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 10.73 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: Can you explain Python funct...\n",
      "\n",
      "--- Test 5: What are the main Python data types? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.466601 seconds\n",
      "Retrying request to /chat/completions in 0.466601 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.791438 seconds\n",
      "Retrying request to /chat/completions in 0.791438 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.808856 seconds\n",
      "Retrying request to /chat/completions in 1.808856 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5819544486058909 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5819544486058909 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.382295 seconds\n",
      "Retrying request to /chat/completions in 0.382295 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.847420 seconds\n",
      "Retrying request to /chat/completions in 0.847420 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.826696 seconds\n",
      "Retrying request to /chat/completions in 1.826696 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.3794849998837 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.3794849998837 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.471321 seconds\n",
      "Retrying request to /chat/completions in 0.471321 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.889050 seconds\n",
      "Retrying request to /chat/completions in 0.889050 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.596100 seconds\n",
      "Retrying request to /chat/completions in 1.596100 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 11.48 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: What are the main Python dat...\n",
      "\n",
      "============================================================\n",
      "TEST REPORT\n",
      "============================================================\n",
      "Total tests: 5\n",
      "Successful tests: 5\n",
      "Failed tests: 0\n",
      "Average response time: 12.73 seconds\n",
      "\n",
      "--- DOCUMENTED ISSUES ---\n",
      "• Slow response time: 12.73s average\n",
      "\n",
      "Step 14: Creating Gradio Interface...\n",
      "  ✓ Gradio interface created\n",
      "\n",
      "============================================================\n",
      "🎉 RAG-based LLM Tutor is ready!\n",
      "============================================================\n",
      "The system includes:\n",
      "✅ Vector database with your knowledge base\n",
      "✅ RAG retrieval and generation\n",
      "✅ Conversation context handling\n",
      "✅ Interactive Gradio interface\n",
      "✅ Comprehensive testing\n",
      "\n",
      "🌐 Launching Gradio interface...\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the complete RAG tutor system\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Complete RAG-based LLM Tutor...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 11: Initialize RAG Tutor\n",
    "        rag_tutor = RAGTutor()\n",
    "        \n",
    "        # Step 12: Setup LlamaIndex RAG\n",
    "        index = setup_llama_index_rag(rag_tutor)\n",
    "        if index is None:\n",
    "            print(\"❌ Failed to setup LlamaIndex RAG\")\n",
    "            return\n",
    "            \n",
    "        query_engine = create_query_engine(index)\n",
    "        \n",
    "        # Step 13: Initialize context manager\n",
    "        print(\"\\nStep 13: Initializing conversation context...\")\n",
    "        context_manager = ConversationContext(max_history=5)\n",
    "        print(\"  ✓ Context manager initialized\")\n",
    "        \n",
    "        # Step 15: Test the system\n",
    "        test_results = test_rag_system(rag_tutor, query_engine, context_manager)\n",
    "        \n",
    "        # Step 14: Create and launch Gradio interface\n",
    "        interface = create_gradio_interface(rag_tutor, query_engine, context_manager)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎉 RAG-based LLM Tutor is ready!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"The system includes:\")\n",
    "        print(\"✅ Vector database with your knowledge base\")\n",
    "        print(\"✅ RAG retrieval and generation\")\n",
    "        print(\"✅ Conversation context handling\")\n",
    "        print(\"✅ Interactive Gradio interface\")\n",
    "        print(\"✅ Comprehensive testing\")\n",
    "        \n",
    "        # Launch the interface\n",
    "        print(\"\\n🌐 Launching Gradio interface...\")\n",
    "        interface.launch(\n",
    "            share=False,  # Set to True if you want a public link\n",
    "            debug=True,\n",
    "            server_port=7860\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in main execution: {e}\")\n",
    "        print(\"Please check your ChromaDB setup and ensure the knowledge base is properly created.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d875767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Complete RAG-based LLM Tutor...\n",
      "Step 11: Setting up RAG Framework...\n",
      "  - Initializing embedding model...\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "  ✓ Embedding model initialized\n",
      "  - Connecting to ChromaDB...\n",
      "  ✓ Connected to ChromaDB collection with 45 documents\n",
      "  - Setting up LLM pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ LLM pipeline initialized\n",
      "✓ RAG Framework setup complete!\n",
      "\n",
      "Step 12: Setting up LlamaIndex RAG Integration...\n",
      "  ✓ LlamaIndex integration setup complete\n",
      "  - Creating query engine...\n",
      "  ✓ Query engine created\n",
      "\n",
      "Step 13: Initializing conversation context...\n",
      "  ✓ Context manager initialized\n",
      "\n",
      "Step 15: Testing RAG System...\n",
      "\n",
      "--- Test 1: What are Python variables? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.490761 seconds\n",
      "Retrying request to /chat/completions in 0.490761 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.945531 seconds\n",
      "Retrying request to /chat/completions in 0.945531 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.816470 seconds\n",
      "Retrying request to /chat/completions in 1.816470 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.638638085419002 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.638638085419002 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.414073 seconds\n",
      "Retrying request to /chat/completions in 0.414073 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.767657 seconds\n",
      "Retrying request to /chat/completions in 0.767657 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.783153 seconds\n",
      "Retrying request to /chat/completions in 1.783153 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.4091776012022348 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.4091776012022348 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.401726 seconds\n",
      "Retrying request to /chat/completions in 0.401726 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.804226 seconds\n",
      "Retrying request to /chat/completions in 0.804226 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.654938 seconds\n",
      "Retrying request to /chat/completions in 1.654938 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 12.47 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: What are Python variables?. ...\n",
      "\n",
      "--- Test 2: How do I create a list in Python? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.413031 seconds\n",
      "Retrying request to /chat/completions in 0.413031 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.796060 seconds\n",
      "Retrying request to /chat/completions in 0.796060 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.510082 seconds\n",
      "Retrying request to /chat/completions in 1.510082 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5126119320704075 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.5126119320704075 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.434224 seconds\n",
      "Retrying request to /chat/completions in 0.434224 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.828675 seconds\n",
      "Retrying request to /chat/completions in 0.828675 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.547023 seconds\n",
      "Retrying request to /chat/completions in 1.547023 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.47105619949416067 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.47105619949416067 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.425166 seconds\n",
      "Retrying request to /chat/completions in 0.425166 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.971255 seconds\n",
      "Retrying request to /chat/completions in 0.971255 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.972392 seconds\n",
      "Retrying request to /chat/completions in 1.972392 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 12.05 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: How do I create a list in Py...\n",
      "\n",
      "--- Test 3: What is a for loop? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.477393 seconds\n",
      "Retrying request to /chat/completions in 0.477393 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.838386 seconds\n",
      "Retrying request to /chat/completions in 0.838386 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.913810 seconds\n",
      "Retrying request to /chat/completions in 1.913810 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.1377354842639258 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.1377354842639258 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.386825 seconds\n",
      "Retrying request to /chat/completions in 0.386825 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.859191 seconds\n",
      "Retrying request to /chat/completions in 0.859191 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.907514 seconds\n",
      "Retrying request to /chat/completions in 1.907514 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 1.6101186391439999 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.6101186391439999 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.476969 seconds\n",
      "Retrying request to /chat/completions in 0.476969 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.871782 seconds\n",
      "Retrying request to /chat/completions in 0.871782 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.990596 seconds\n",
      "Retrying request to /chat/completions in 1.990596 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 13.54 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: What is a for loop?. Please ...\n",
      "\n",
      "--- Test 4: Can you explain Python functions? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.411627 seconds\n",
      "Retrying request to /chat/completions in 0.411627 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.760234 seconds\n",
      "Retrying request to /chat/completions in 0.760234 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.932094 seconds\n",
      "Retrying request to /chat/completions in 1.932094 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.018055875104321717 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.018055875104321717 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.485402 seconds\n",
      "Retrying request to /chat/completions in 0.485402 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.751600 seconds\n",
      "Retrying request to /chat/completions in 0.751600 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.587372 seconds\n",
      "Retrying request to /chat/completions in 1.587372 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 1.188141428790931 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.188141428790931 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.440742 seconds\n",
      "Retrying request to /chat/completions in 0.440742 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.888495 seconds\n",
      "Retrying request to /chat/completions in 0.888495 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.900142 seconds\n",
      "Retrying request to /chat/completions in 1.900142 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 12.07 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: Can you explain Python funct...\n",
      "\n",
      "--- Test 5: What are the main Python data types? ---\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.499306 seconds\n",
      "Retrying request to /chat/completions in 0.499306 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.814273 seconds\n",
      "Retrying request to /chat/completions in 0.814273 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.849518 seconds\n",
      "Retrying request to /chat/completions in 1.849518 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.7490623565527189 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.7490623565527189 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.469419 seconds\n",
      "Retrying request to /chat/completions in 0.469419 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.849326 seconds\n",
      "Retrying request to /chat/completions in 0.849326 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.599409 seconds\n",
      "Retrying request to /chat/completions in 1.599409 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 1.897946681975285 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.897946681975285 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.401865 seconds\n",
      "Retrying request to /chat/completions in 0.401865 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.876126 seconds\n",
      "Retrying request to /chat/completions in 0.876126 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.570592 seconds\n",
      "Retrying request to /chat/completions in 1.570592 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Response time: 13.52 seconds\n",
      "Response preview: I apologize, but I encountered an error processing your question about: What are the main Python dat...\n",
      "\n",
      "============================================================\n",
      "TEST REPORT\n",
      "============================================================\n",
      "Total tests: 5\n",
      "Successful tests: 5\n",
      "Failed tests: 0\n",
      "Average response time: 12.73 seconds\n",
      "\n",
      "--- DOCUMENTED ISSUES ---\n",
      "• Slow response time: 12.73s average\n",
      "\n",
      "Step 14: Creating Gradio Interface...\n",
      "  ✓ Gradio interface created\n",
      "\n",
      "============================================================\n",
      "🎉 RAG-based LLM Tutor is ready!\n",
      "============================================================\n",
      "The system includes:\n",
      "✅ Vector database with your knowledge base\n",
      "✅ RAG retrieval and generation\n",
      "✅ Conversation context handling\n",
      "✅ Interactive Gradio interface\n",
      "✅ Comprehensive testing\n",
      "\n",
      "🌐 Launching Gradio interface...\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.418900 seconds\n",
      "Retrying request to /chat/completions in 0.418900 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.821086 seconds\n",
      "Retrying request to /chat/completions in 0.821086 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.702065 seconds\n",
      "Retrying request to /chat/completions in 1.702065 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.3701594276213507 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.3701594276213507 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.400364 seconds\n",
      "Retrying request to /chat/completions in 0.400364 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.833724 seconds\n",
      "Retrying request to /chat/completions in 0.833724 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.546548 seconds\n",
      "Retrying request to /chat/completions in 1.546548 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.3267514152930482 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.3267514152930482 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.439948 seconds\n",
      "Retrying request to /chat/completions in 0.439948 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.953696 seconds\n",
      "Retrying request to /chat/completions in 0.953696 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.590510 seconds\n",
      "Retrying request to /chat/completions in 1.590510 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "\n",
      "Step 13: Processing query with context...\n",
      "  - Retrieving relevant information...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.440257 seconds\n",
      "Retrying request to /chat/completions in 0.440257 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.925508 seconds\n",
      "Retrying request to /chat/completions in 0.925508 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.792905 seconds\n",
      "Retrying request to /chat/completions in 1.792905 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.9705281951689 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.9705281951689 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.424744 seconds\n",
      "Retrying request to /chat/completions in 0.424744 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.838916 seconds\n",
      "Retrying request to /chat/completions in 0.838916 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.803369 seconds\n",
      "Retrying request to /chat/completions in 1.803369 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "WARNING:llama_index.llms.openai.utils:Retrying llama_index.llms.openai.base.OpenAI._chat in 0.685614151460247 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.685614151460247 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.377957 seconds\n",
      "Retrying request to /chat/completions in 0.377957 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.953116 seconds\n",
      "Retrying request to /chat/completions in 0.953116 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.611330 seconds\n",
      "Retrying request to /chat/completions in 1.611330 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "  ✗ Error processing query: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ADDITIONAL UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def quick_test_query(question=\"What are Python variables?\"):\n",
    "    \"\"\"\n",
    "    Quick function to test a single query without launching full interface\n",
    "    \"\"\"\n",
    "    print(f\"Quick test query: {question}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize components\n",
    "        rag_tutor = RAGTutor()\n",
    "        index = setup_llama_index_rag(rag_tutor)\n",
    "        query_engine = create_query_engine(index)\n",
    "        context_manager = ConversationContext()\n",
    "        \n",
    "        # Test query\n",
    "        response = enhanced_query_with_context(query_engine, question, context_manager)\n",
    "        \n",
    "        print(f\"Response: {response}\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in quick test: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_system_status():\n",
    "    \"\"\"\n",
    "    Check if all components are working correctly\n",
    "    \"\"\"\n",
    "    print(\"Checking system status...\")\n",
    "    \n",
    "    status = {\n",
    "        'chromadb_connection': False,\n",
    "        'embedding_model': False,\n",
    "        'collection_count': 0,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check ChromaDB connection\n",
    "        client = chromadb.PersistentClient(path=\"../data/chroma_db\")\n",
    "        collection = client.get_collection(name=\"python_tutor\")\n",
    "        status['chromadb_connection'] = True\n",
    "        status['collection_count'] = collection.count()\n",
    "        \n",
    "        # Check embedding model\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        status['embedding_model'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        status['issues'].append(f\"System check error: {e}\")\n",
    "    \n",
    "    # Print status\n",
    "    print(f\"ChromaDB Connection: {'✅' if status['chromadb_connection'] else '❌'}\")\n",
    "    print(f\"Embedding Model: {'✅' if status['embedding_model'] else '❌'}\")\n",
    "    print(f\"Documents in collection: {status['collection_count']}\")\n",
    "    \n",
    "    if status['issues']:\n",
    "        print(\"Issues found:\")\n",
    "        for issue in status['issues']:\n",
    "            print(f\"  • {issue}\")\n",
    "    else:\n",
    "        print(\"✅ All systems operational!\")\n",
    "        \n",
    "    return status\n",
    "\n",
    "# Uncomment the following lines to run specific components:\n",
    "# check_system_status()\n",
    "# quick_test_query(\"What are Python data types?\")\n",
    "main()  # Run the full system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
