{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# %pip install -qU llama-index\n",
        "# %pip install -qU llama-index-llms-mistralai\n",
        "# %pip install -qU llama-index-embeddings-mistralai\n",
        "# %pip install -qU llama-index-vector-stores-chroma\n",
        "# %pip install -qU pypdf beautifulsoup4 requests chromadb gradio nltk numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import requests\n",
        "import nltk\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, Document, StorageContext\n",
        "from llama_index.llms.mistralai import MistralAI\n",
        "# from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "import gradio as gr\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d48bb77b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key configured successfully from .env file\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set up Mistral API key from environment variables\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "if not MISTRAL_API_KEY:\n",
        "    print(\"MISTRAL_API_KEY not found. Please set it in your .env file.\")\n",
        "else:\n",
        "    os.environ[\"MISTRAL_API_KEY\"] = MISTRAL_API_KEY\n",
        "    print(\"API key configured successfully from .env file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context handler initialized\n"
          ]
        }
      ],
      "source": [
        "# Context Handler Class for Basic Context Handling\n",
        "class ContextHandler:\n",
        "    def __init__(self, max_history=5):\n",
        "        self.conversation_history = []\n",
        "        self.max_history = max_history\n",
        "    \n",
        "    def add_to_history(self, question: str, answer: str):\n",
        "        \"\"\"Add Q&A pair to conversation history\"\"\"\n",
        "        self.conversation_history.append({\n",
        "            'question': question,\n",
        "            'answer': answer\n",
        "        })\n",
        "        \n",
        "        # Keep only recent history\n",
        "        if len(self.conversation_history) > self.max_history:\n",
        "            self.conversation_history = self.conversation_history[-self.max_history:]\n",
        "    \n",
        "    def get_context_string(self) -> str:\n",
        "        \"\"\"Get formatted context string for the LLM\"\"\"\n",
        "        if not self.conversation_history:\n",
        "            return \"\"\n",
        "        \n",
        "        context = \"\\nPrevious conversation context:\\n\"\n",
        "        for i, entry in enumerate(self.conversation_history, 1):\n",
        "            context += f\"Q{i}: {entry['question']}\\n\"\n",
        "            context += f\"A{i}: {entry['answer'][:200]}...\\n\\n\"  # Truncate long answers\n",
        "        \n",
        "        return context\n",
        "    \n",
        "    def clear_history(self):\n",
        "        \"\"\"Clear conversation history\"\"\"\n",
        "        self.conversation_history = []\n",
        "\n",
        "# Initialize context handler\n",
        "context_handler = ContextHandler()\n",
        "print(\"Context handler initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fce69a",
      "metadata": {},
      "source": [
        "Models used:\n",
        "* Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
        "* LLM model: codestral 25.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM and embedding models initialized with enhanced system prompt\n",
            "Embedding model dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# Initialize LLM and Embedding models with enhanced system prompt\n",
        "system_prompt = \"\"\"You are an expert Python programming tutor and assistant. You help students learn Python programming concepts, explain code, debug issues, and provide clear, practical examples. \n",
        "When answering questions:\n",
        "- Provide clear, step-by-step explanations\n",
        "- Include relevant code examples when appropriate\n",
        "- Explain concepts in simple terms for beginners\n",
        "- If you don't know something from the provided context, say so clearly\n",
        "- Focus on practical, hands-on learning\n",
        "- Consider the conversation history when relevant to provide contextual responses\n",
        "- Build upon previous questions and answers when appropriate\"\"\"\n",
        "\n",
        "llm = MistralAI(\n",
        "    model=\"codestral-latest\", \n",
        "    api_key=MISTRAL_API_KEY,\n",
        "    system_prompt=system_prompt\n",
        ")\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "print(\"LLM and embedding models initialized with enhanced system prompt\")\n",
        "print(f\"Embedding model dimension: {embed_model._model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped content length: 58391 characters\n"
          ]
        }
      ],
      "source": [
        "# Web scraping function using BeautifulSoup\n",
        "def scrape_website(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        \n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\"]):\n",
        "            script.decompose()\n",
        "        \n",
        "        text = soup.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Scrape Python tutorial website\n",
        "website_url = \"https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\"\n",
        "web_content = scrape_website(website_url)\n",
        "\n",
        "print(f\"Scraped content length: {len(web_content)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Web content preprocessed successfully\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Preserve code blocks (anything between triple backticks or indented blocks)\n",
        "    code_pattern = r'```[\\s\\S]*?```|(?:^|\\n)(?: {4,}|\\t)[^\\n]*(?:\\n(?: {4,}|\\t)[^\\n]*)*'\n",
        "    code_blocks = re.findall(code_pattern, text, re.MULTILINE)\n",
        "    \n",
        "    # Replace code blocks with placeholders\n",
        "    for i, block in enumerate(code_blocks):\n",
        "        text = text.replace(block, f\"__CODE_BLOCK_{i}__\")\n",
        "    \n",
        "    # Clean text (remove extra whitespace, normalize)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Multiple whitespace to single space\n",
        "    text = re.sub(r'\\n+', '\\n', text)  # Multiple newlines to single\n",
        "    text = text.strip()\n",
        "    \n",
        "    # Restore code blocks\n",
        "    for i, block in enumerate(code_blocks):\n",
        "        text = text.replace(f\"__CODE_BLOCK_{i}__\", block)\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Preprocess web content\n",
        "if web_content:\n",
        "    web_content = preprocess_text(web_content)\n",
        "    print(\"Web content preprocessed successfully\")\n",
        "else:\n",
        "    print(\"No web content to preprocess\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded and preprocessed 1310 PDF documents\n",
            "Added web content as document\n",
            "Total documents: 1311\n"
          ]
        }
      ],
      "source": [
        "# Load PDF documents and combine with web content\n",
        "pdf_folder_path = \"../data/pdfs\"\n",
        "documents = []\n",
        "\n",
        "# Load PDFs\n",
        "if os.path.exists(pdf_folder_path):\n",
        "    pdf_docs = SimpleDirectoryReader(pdf_folder_path).load_data()\n",
        "    # Preprocess PDF content\n",
        "    preprocessed_docs = []\n",
        "    for doc in pdf_docs:\n",
        "        new_doc = Document(\n",
        "            text=preprocess_text(doc.text),\n",
        "            metadata=doc.metadata\n",
        "        )\n",
        "        preprocessed_docs.append(new_doc)\n",
        "    documents.extend(preprocessed_docs)\n",
        "    print(f\"Loaded and preprocessed {len(pdf_docs)} PDF documents\")\n",
        "else:\n",
        "    print(\"PDF folder not found\")\n",
        "\n",
        "# Add web content as document\n",
        "if web_content:\n",
        "    web_doc = Document(\n",
        "        text=web_content,\n",
        "        metadata={\"source\": \"web\", \"url\": website_url}\n",
        "    )\n",
        "    documents.append(web_doc)\n",
        "    print(\"Added web content as document\")\n",
        "\n",
        "print(f\"Total documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SAMPLE DOCUMENT CHUNKS AFTER EXTRACTION AND CLEANING ===\n",
            "Displaying 3 sample chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: Unknown\n",
            "File: Python Crash Course.pdf\n",
            "Text length: 2046 characters\n",
            "Text preview (first 500 chars):\n",
            "A HANDS-ON , PROJECT-BASED INTRODUCTION TO PROGRAMMING ERIC MATTHES P Y THON C R ASH COURSE P Y THON C R ASH COURSE SHELVE IN: PROGRAMMING LANGUAGES/ PYTHON $39.95 ($45.95 CDN) FAST! LEARN PYTHON— FAST! LEARN PYTHON— PYTHON CRASH COURSEPYTHON CRASH COURSEMATTHES COVERS PYTHON 2 AND 3 Python Crash Course is a fast-paced, thorough intro- duction to programming with Python that will have you writing programs, solving problems, and making things that work in no time. In the first half of the book, y...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: Unknown\n",
            "File: Python Crash Course.pdf\n",
            "Text length: 19 characters\n",
            "Text preview (first 500 chars):\n",
            "Python Crash Course...\n",
            "\n",
            "================================================================================\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: Unknown\n",
            "File: Python Crash Course.pdf\n",
            "Text length: 0 characters\n",
            "Text preview (first 500 chars):\n",
            "...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display sample chunks after extraction and cleaning\n",
        "def display_sample_chunks(documents, num_samples=3):\n",
        "    print(\"=== SAMPLE DOCUMENT CHUNKS AFTER EXTRACTION AND CLEANING ===\")\n",
        "    print(f\"Displaying {min(num_samples, len(documents))} sample chunks:\\n\")\n",
        "    \n",
        "    for i, doc in enumerate(documents[:num_samples]):\n",
        "        print(f\"--- Chunk {i+1} ---\")\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
        "        if 'file_name' in doc.metadata:\n",
        "            print(f\"File: {doc.metadata['file_name']}\")\n",
        "        if 'url' in doc.metadata:\n",
        "            print(f\"URL: {doc.metadata['url']}\")\n",
        "        print(f\"Text length: {len(doc.text)} characters\")\n",
        "        print(f\"Text preview (first 500 chars):\\n{doc.text[:500]}...\")\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Display sample chunks\n",
        "if documents:\n",
        "    display_sample_chunks(documents)\n",
        "else:\n",
        "    print(\"No documents loaded to display\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB setup completed\n",
            "Collection count: 18\n"
          ]
        }
      ],
      "source": [
        "# Setup ChromaDB for persistence\n",
        "chroma_client = chromadb.PersistentClient(path=\"../data/chroma_db2\")\n",
        "chroma_collection = chroma_client.get_or_create_collection(\"python_docs\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "print(\"ChromaDB setup completed\")\n",
        "print(f\"Collection count: {chroma_collection.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Existing vector index loaded from cache\n",
            "Index contains 18 document chunks\n"
          ]
        }
      ],
      "source": [
        "# Create or load vector index with caching\n",
        "if chroma_collection.count() == 0:\n",
        "    # Create new index if empty\n",
        "    vector_index = VectorStoreIndex.from_documents(\n",
        "        documents, \n",
        "        storage_context=storage_context,\n",
        "        show_progress=True\n",
        "    )\n",
        "    print(\"New vector index created and persisted\")\n",
        "else:\n",
        "    # Load existing index\n",
        "    vector_index = VectorStoreIndex.from_vector_store(\n",
        "        vector_store=vector_store,\n",
        "        storage_context=storage_context\n",
        "    )\n",
        "    print(\"Existing vector index loaded from cache\")\n",
        "\n",
        "print(f\"Index contains {chroma_collection.count()} document chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SAMPLE EMBEDDINGS AND THEIR SHAPE ===\n",
            "Sample text 1: A HANDS-ON , PROJECT-BASED INTRODUCTION TO PROGRAMMING ERIC MATTHES P Y THON C R ASH COURSE P Y THON C R ASH COURSE SHELVE IN: PROGRAMMING LANGUAGES/ PYTHON $39.95 ($45.95 CDN) FAST! LEARN PYTHON— FAS...\n",
            "\n",
            "Sample text 2: Python Crash Course...\n",
            "\n",
            "Number of embeddings generated: 2\n",
            "\n",
            "Embedding 1:\n",
            "  Shape: (384,)\n",
            "  Type: <class 'numpy.ndarray'>\n",
            "  Min value: -0.1371\n",
            "  Max value: 0.1886\n",
            "  Mean value: -0.0010\n",
            "  First 10 values: [-0.08057992  0.00041429 -0.01473134 -0.01707213 -0.06654104 -0.10962446\n",
            "  0.05017243  0.04082484 -0.09665311  0.02742115]\n",
            "  Last 10 values: [ 1.07379861e-01  5.47668785e-02 -9.17729437e-02 -5.07228906e-05\n",
            " -3.57930772e-02  4.36313786e-02  1.64995305e-02  6.50759786e-02\n",
            "  1.06741174e-03  6.25112876e-02]\n",
            "\n",
            "Embedding 2:\n",
            "  Shape: (384,)\n",
            "  Type: <class 'numpy.ndarray'>\n",
            "  Min value: -0.1464\n",
            "  Max value: 0.1609\n",
            "  Mean value: 0.0008\n",
            "  First 10 values: [ 0.02638219 -0.03466475 -0.00692113 -0.00566348 -0.00317804 -0.09809611\n",
            "  0.01400697  0.06130115 -0.09240743  0.00536713]\n",
            "  Last 10 values: [ 0.10907034  0.03352599 -0.05589331  0.0087619  -0.03801813  0.0124807\n",
            " -0.01032302  0.0896912   0.05813155 -0.00168805]\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display sample embeddings and their shape\n",
        "def display_sample_embeddings(num_samples=2):\n",
        "    print(\"=== SAMPLE EMBEDDINGS AND THEIR SHAPE ===\")\n",
        "    \n",
        "    # Get sample texts from documents\n",
        "    sample_texts = []\n",
        "    for i, doc in enumerate(documents[:num_samples]):\n",
        "        # Take first 200 characters as sample\n",
        "        sample_text = doc.text[:200].strip()\n",
        "        sample_texts.append(sample_text)\n",
        "        print(f\"Sample text {i+1}: {sample_text}...\\n\")\n",
        "    \n",
        "    if sample_texts:\n",
        "        # Generate embeddings for sample texts\n",
        "        embeddings = embed_model.get_text_embedding_batch(sample_texts)\n",
        "        \n",
        "        print(f\"Number of embeddings generated: {len(embeddings)}\")\n",
        "        \n",
        "        for i, embedding in enumerate(embeddings):\n",
        "            embedding_array = np.array(embedding)\n",
        "            print(f\"\\nEmbedding {i+1}:\")\n",
        "            print(f\"  Shape: {embedding_array.shape}\")\n",
        "            print(f\"  Type: {type(embedding_array)}\")\n",
        "            print(f\"  Min value: {embedding_array.min():.4f}\")\n",
        "            print(f\"  Max value: {embedding_array.max():.4f}\")\n",
        "            print(f\"  Mean value: {embedding_array.mean():.4f}\")\n",
        "            print(f\"  First 10 values: {embedding_array[:10]}\")\n",
        "            print(f\"  Last 10 values: {embedding_array[-10:]}\")\n",
        "    else:\n",
        "        print(\"No documents available for embedding demonstration\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Display sample embeddings\n",
        "if documents:\n",
        "    display_sample_embeddings()\n",
        "else:\n",
        "    print(\"No documents available for embedding demonstration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query engine created with default indexing\n"
          ]
        }
      ],
      "source": [
        "# Create query engine with default indexing\n",
        "query_engine = vector_index.as_query_engine(\n",
        "    similarity_top_k=3,  # Return top 3 most similar chunks\n",
        "    response_mode=\"compact\",  # Compact response format\n",
        ")\n",
        "\n",
        "print(\"Query engine created with default indexing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced query functions defined with context handling\n"
          ]
        }
      ],
      "source": [
        "# Sample questions for the interface\n",
        "sample_questions = [\n",
        "    \"What is a Python function?\",\n",
        "    \"How do you define and call a void function?\",\n",
        "    \"Explain Python data types\",\n",
        "    \"What are Python loops?\",\n",
        "    \"How to handle errors in Python?\",\n",
        "    \"Explain Python classes and objects\"\n",
        "]\n",
        "\n",
        "# Enhanced function to handle queries with context\n",
        "def ask_question(question, use_context=True):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\", \"\"\n",
        "    \n",
        "    try:\n",
        "        # Prepare query with context if enabled\n",
        "        if use_context:\n",
        "            context_string = context_handler.get_context_string()\n",
        "            enhanced_question = f\"{context_string}\\nCurrent question: {question}\"\n",
        "        else:\n",
        "            enhanced_question = question\n",
        "        \n",
        "        response = query_engine.query(enhanced_question)\n",
        "        answer = str(response)\n",
        "        \n",
        "        # Add to conversation history\n",
        "        if use_context:\n",
        "            context_handler.add_to_history(question, answer)\n",
        "        \n",
        "        # Format context info\n",
        "        context_info = f\"Context enabled: {use_context}\\n\"\n",
        "        context_info += f\"History length: {len(context_handler.conversation_history)} items\\n\"\n",
        "        if context_handler.conversation_history:\n",
        "            context_info += f\"Last question: {context_handler.conversation_history[-1]['question'][:50]}...\"\n",
        "        \n",
        "        return answer, context_info\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error processing question: {e}\", f\"Error occurred: {e}\"\n",
        "\n",
        "# Function for clearing context\n",
        "def clear_context():\n",
        "    context_handler.clear_history()\n",
        "    return \"Context cleared successfully!\", \"Context cleared - no conversation history\"\n",
        "\n",
        "print(\"Enhanced query functions defined with context handling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enhanced Gradio interface created with context handling\n"
          ]
        }
      ],
      "source": [
        "# Create Enhanced Gradio interface that runs internally\n",
        "def create_interface():\n",
        "    # Create sample questions HTML\n",
        "    sample_html = \"<h3>Try asking:</h3><ul>\"\n",
        "    for q in sample_questions:\n",
        "        sample_html += f\"<li>{q}</li>\"\n",
        "    sample_html += \"</ul>\"\n",
        "    \n",
        "    with gr.Blocks(title=\"Python Tutor\", theme=gr.themes.Soft()) as interface:\n",
        "        gr.Markdown(\"# 🐍 Python Tutor\")\n",
        "        gr.Markdown(\"Ask me anything about Python programming! I remember our conversation context.\")\n",
        "        \n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                question_input = gr.Textbox(\n",
        "                    label=\"Your Question\",\n",
        "                    placeholder=\"Enter your Python question here...\",\n",
        "                    lines=3\n",
        "                )\n",
        "                \n",
        "                with gr.Row():\n",
        "                    submit_btn = gr.Button(\"Ask Question\", variant=\"primary\", scale=2)\n",
        "                    context_toggle = gr.Checkbox(\n",
        "                        label=\"Use Context\", \n",
        "                        value=True, \n",
        "                        info=\"Remember conversation history\",\n",
        "                        scale=1\n",
        "                    )\n",
        "                    clear_btn = gr.Button(\"Clear Context\", variant=\"secondary\", scale=1)\n",
        "                \n",
        "            with gr.Column(scale=1):\n",
        "                gr.HTML(sample_html)\n",
        "                \n",
        "                context_info = gr.Textbox(\n",
        "                    label=\"Context Info\",\n",
        "                    lines=4,\n",
        "                    value=\"Context enabled: True\\nHistory length: 0 items\",\n",
        "                    interactive=False\n",
        "                )\n",
        "        \n",
        "        answer_output = gr.Textbox(\n",
        "            label=\"Answer\",\n",
        "            lines=12,\n",
        "            max_lines=25\n",
        "        )\n",
        "        \n",
        "        # Event handlers\n",
        "        submit_btn.click(\n",
        "            fn=ask_question,\n",
        "            inputs=[question_input, context_toggle],\n",
        "            outputs=[answer_output, context_info]\n",
        "        )\n",
        "        \n",
        "        question_input.submit(\n",
        "            fn=ask_question,\n",
        "            inputs=[question_input, context_toggle],\n",
        "            outputs=[answer_output, context_info]\n",
        "        )\n",
        "        \n",
        "        clear_btn.click(\n",
        "            fn=clear_context,\n",
        "            outputs=[answer_output, context_info]\n",
        "        )\n",
        "        \n",
        "        # Example questions as buttons\n",
        "        gr.Markdown(\"### Quick Examples:\")\n",
        "        with gr.Row():\n",
        "            for i in range(0, len(sample_questions), 2):\n",
        "                if i < len(sample_questions):\n",
        "                    btn = gr.Button(sample_questions[i], size=\"sm\")\n",
        "                    btn.click(\n",
        "                        lambda q=sample_questions[i]: q,\n",
        "                        outputs=question_input\n",
        "                    )\n",
        "                if i+1 < len(sample_questions):\n",
        "                    btn = gr.Button(sample_questions[i+1], size=\"sm\")\n",
        "                    btn.click(\n",
        "                        lambda q=sample_questions[i+1]: q,\n",
        "                        outputs=question_input\n",
        "                    )\n",
        "    \n",
        "    return interface\n",
        "\n",
        "# Create interface\n",
        "app = create_interface()\n",
        "print(\"Enhanced Gradio interface created with context handling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Question: What is a Python function?\n",
            "\n",
            "Test Response: A Python function is a block of code that performs a specific task. Functions are used to organize code into reusable pieces, making it easier to read, write, and maintain. They can take inputs, known as arguments, and return outputs. Functions are defined using the `def` keyword followed by the function name and parentheses. Here's a simple example:\n",
            "\n",
            "```python\n",
            "def greet():\n",
            "    print(\"Hello, Geeks!\")\n",
            "\n",
            "greet()  # This will print \"Hello, Geeks!\"\n",
            "```\n",
            "\n",
            "In this example, `greet` is a function that pri...\n",
            "\n",
            "Context Info: Context enabled: True\n",
            "History length: 1 items\n",
            "Last question: What is a Python function?...\n",
            "\n",
            "=== System Ready ===\n",
            "Run the next cell to launch the Gradio interface internally\n"
          ]
        }
      ],
      "source": [
        "# Test the system with a sample question\n",
        "test_question = \"What is a Python function?\"\n",
        "print(f\"Test Question: {test_question}\")\n",
        "test_response, test_context = ask_question(test_question)\n",
        "print(f\"\\nTest Response: {test_response[:500]}...\")\n",
        "print(f\"\\nContext Info: {test_context}\")\n",
        "\n",
        "print(\"\\n=== System Ready ===\")\n",
        "print(\"Run the next cell to launch the Gradio interface internally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Launch Gradio interface internally (runs inside the notebook)\n",
        "app.launch(\n",
        "    share=False,  # Keep it internal\n",
        "    inbrowser=True,  # Open in browser tab\n",
        "    inline=True,  # Display inline in notebook\n",
        "    height=800,   # Set height for inline display\n",
        "    quiet=True    # Reduce output noise\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Test utility functions defined\n",
            "\n",
            "Available test functions:\n",
            "- run_comprehensive_test(): Complete system test\n",
            "- test_embedding_similarity(): Test embedding quality\n",
            "- test_context_memory(): Test context handling limits\n",
            "- run_quick_test(): Quick functionality check\n",
            "\n",
            "Run any of these functions to test the system!\n"
          ]
        }
      ],
      "source": [
        "# Test Utility Function - Comprehensive testing of all steps\n",
        "def run_comprehensive_test():\n",
        "    \"\"\"Test all components of the Python Learning Assistant system\"\"\"\n",
        "    \n",
        "    print(\"COMPREHENSIVE SYSTEM TEST\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    test_results = {\n",
        "        'document_loading': False,\n",
        "        'preprocessing': False,\n",
        "        'embedding': False,\n",
        "        'vector_store': False,\n",
        "        'query_engine': False,\n",
        "        'context_handling': False,\n",
        "        'basic_query': False,\n",
        "        'context_query': False\n",
        "    }\n",
        "    \n",
        "    # Test 1: Document Loading\n",
        "    print(\"\\n1. Testing Document Loading...\")\n",
        "    try:\n",
        "        if documents and len(documents) > 0:\n",
        "            print(f\"    Documents loaded: {len(documents)}\")\n",
        "            test_results['document_loading'] = True\n",
        "        else:\n",
        "            print(\"    No documents loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Document loading error: {e}\")\n",
        "    \n",
        "    # Test 2: Preprocessing\n",
        "    print(\"\\n2. Testing Text Preprocessing...\")\n",
        "    try:\n",
        "        test_text = \"This is a   test\\n\\n\\nwith    multiple spaces.\"\n",
        "        processed = preprocess_text(test_text)\n",
        "        if len(processed) < len(test_text):  # Should be cleaned\n",
        "            print(f\"    Preprocessing working (reduced from {len(test_text)} to {len(processed)} chars)\")\n",
        "            test_results['preprocessing'] = True\n",
        "        else:\n",
        "            print(\"     Preprocessing may not be working optimally\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Preprocessing error: {e}\")\n",
        "    \n",
        "    # Test 3: Embedding Generation\n",
        "    print(\"\\n3. Testing Embedding Generation...\")\n",
        "    try:\n",
        "        test_text = \"Python is a programming language\"\n",
        "        embedding = embed_model.get_text_embedding(test_text)\n",
        "        embedding_array = np.array(embedding)\n",
        "        print(f\"    Embedding generated: shape {embedding_array.shape}\")\n",
        "        print(f\"    Stats: min={embedding_array.min():.4f}, max={embedding_array.max():.4f}, mean={embedding_array.mean():.4f}\")\n",
        "        test_results['embedding'] = True\n",
        "    except Exception as e:\n",
        "        print(f\"    Embedding error: {e}\")\n",
        "    \n",
        "    # Test 4: Vector Store\n",
        "    print(\"\\n4. Testing Vector Store...\")\n",
        "    try:\n",
        "        collection_count = chroma_collection.count()\n",
        "        if collection_count > 0:\n",
        "            print(f\"    Vector store working: {collection_count} vectors stored\")\n",
        "            test_results['vector_store'] = True\n",
        "        else:\n",
        "            print(\"    Vector store empty\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Vector store error: {e}\")\n",
        "    \n",
        "    # Test 5: Query Engine\n",
        "    print(\"\\n5. Testing Query Engine...\")\n",
        "    try:\n",
        "        if query_engine:\n",
        "            print(\"    Query engine initialized\")\n",
        "            test_results['query_engine'] = True\n",
        "        else:\n",
        "            print(\"    Query engine not initialized\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Query engine error: {e}\")\n",
        "    \n",
        "    # Test 6: Context Handling\n",
        "    print(\"\\n6. Testing Context Handling...\")\n",
        "    try:\n",
        "        # Clear context first\n",
        "        context_handler.clear_history()\n",
        "        \n",
        "        # Add test conversation\n",
        "        context_handler.add_to_history(\"What is Python?\", \"Python is a programming language\")\n",
        "        context_handler.add_to_history(\"What are variables?\", \"Variables store data values\")\n",
        "        \n",
        "        context_string = context_handler.get_context_string()\n",
        "        history_length = len(context_handler.conversation_history)\n",
        "        \n",
        "        if history_length == 2 and len(context_string) > 0:\n",
        "            print(f\"    Context handling working: {history_length} items in history\")\n",
        "            print(f\"    Context string length: {len(context_string)} characters\")\n",
        "            test_results['context_handling'] = True\n",
        "        else:\n",
        "            print(f\"    Context handling issue: {history_length} items, context length: {len(context_string)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Context handling error: {e}\")\n",
        "    \n",
        "    # Test 7: Basic Query (without context)\n",
        "    print(\"\\n7. Testing Basic Query (no context)...\")\n",
        "    try:\n",
        "        context_handler.clear_history()  # Clear for clean test\n",
        "        test_question = \"What is a Python function?\"\n",
        "        response, context_info = ask_question(test_question, use_context=False)\n",
        "        \n",
        "        if response and \"Error\" not in response and len(response) > 50:\n",
        "            print(f\"    Basic query working: response length {len(response)} chars\")\n",
        "            print(f\"    Sample response: {response[:100]}...\")\n",
        "            test_results['basic_query'] = True\n",
        "        else:\n",
        "            print(f\"    Basic query failed: {response[:100]}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Basic query error: {e}\")\n",
        "    \n",
        "    # Test 8: Context-Aware Query\n",
        "    print(\"\\n8. Testing Context-Aware Query...\")\n",
        "    try:\n",
        "        # First question to establish context\n",
        "        response1, _ = ask_question(\"What are Python data types?\", use_context=True)\n",
        "        \n",
        "        # Follow-up question that should use context\n",
        "        response2, context_info = ask_question(\"Can you give me examples of these?\", use_context=True)\n",
        "        \n",
        "        if response2 and \"Error\" not in response2 and len(context_handler.conversation_history) >= 2:\n",
        "            print(f\"    Context-aware query working: {len(context_handler.conversation_history)} items in history\")\n",
        "            print(f\"    Follow-up response: {response2[:100]}...\")\n",
        "            print(f\"    Context info: {context_info}\")\n",
        "            test_results['context_query'] = True\n",
        "        else:\n",
        "            print(f\"    Context-aware query failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Context-aware query error: {e}\")\n",
        "    \n",
        "    # Test Summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\" TEST SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    passed_tests = sum(test_results.values())\n",
        "    total_tests = len(test_results)\n",
        "    \n",
        "    for test_name, result in test_results.items():\n",
        "        status = \" PASS\" if result else \"❌ FAIL\"\n",
        "        print(f\"{test_name.replace('_', ' ').title():<25} {status}\")\n",
        "    \n",
        "    print(f\"\\nOverall Result: {passed_tests}/{total_tests} tests passed\")\n",
        "    \n",
        "    if passed_tests == total_tests:\n",
        "        print(\"🎉 ALL TESTS PASSED! System is fully functional.\")\n",
        "    elif passed_tests >= total_tests * 0.75:\n",
        "        print(\"  Most tests passed. System is mostly functional with minor issues.\")\n",
        "    else:\n",
        "        print(\" Multiple test failures. System needs debugging.\")\n",
        "    \n",
        "    # Performance metrics\n",
        "    print(\"\\n PERFORMANCE METRICS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    try:\n",
        "        # Embedding performance test\n",
        "        import time\n",
        "        test_texts = [\"Python function\", \"Variable declaration\", \"Loop iteration\"]\n",
        "        start_time = time.time()\n",
        "        embeddings = embed_model.get_text_embedding_batch(test_texts)\n",
        "        embedding_time = time.time() - start_time\n",
        "        print(f\"Embedding speed: {len(test_texts)/embedding_time:.2f} texts/second\")\n",
        "        \n",
        "        # Query performance test\n",
        "        start_time = time.time()\n",
        "        response, _ = ask_question(\"What is Python?\", use_context=False)\n",
        "        query_time = time.time() - start_time\n",
        "        print(f\"Query response time: {query_time:.2f} seconds\")\n",
        "        \n",
        "        # Memory usage (approximate)\n",
        "        if documents:\n",
        "            total_chars = sum(len(doc.text) for doc in documents)\n",
        "            print(f\"Document corpus size: {total_chars:,} characters\")\n",
        "        \n",
        "        print(f\"Vector store size: {chroma_collection.count():,} vectors\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Performance metrics error: {e}\")\n",
        "    \n",
        "    print(\"\\n RECOMMENDATIONS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    if not test_results['document_loading']:\n",
        "        print(\"- Check PDF folder path and ensure documents are available\")\n",
        "    if not test_results['vector_store']:\n",
        "        print(\"- Verify ChromaDB setup and document indexing\")\n",
        "    if not test_results['basic_query']:\n",
        "        print(\"- Check LLM API key and connection\")\n",
        "    if not test_results['context_query']:\n",
        "        print(\"- Verify context handling implementation\")\n",
        "    \n",
        "    if passed_tests == total_tests:\n",
        "        print(\"- System is optimally configured!\")\n",
        "        print(\"- Consider adding more documents to improve knowledge base\")\n",
        "        print(\"- Monitor query performance and adjust similarity_top_k if needed\")\n",
        "    \n",
        "    return test_results\n",
        "\n",
        "# Additional utility functions for testing individual components\n",
        "\n",
        "def test_embedding_similarity():\n",
        "    \"\"\"Test embedding similarity between related texts\"\"\"\n",
        "    print(\"\\n EMBEDDING SIMILARITY TEST\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    test_pairs = [\n",
        "        (\"Python function\", \"Python method\"),\n",
        "        (\"for loop\", \"while loop\"),\n",
        "        (\"variable\", \"constant\"),\n",
        "        (\"Python\", \"Java\")  # Should be less similar\n",
        "    ]\n",
        "    \n",
        "    for text1, text2 in test_pairs:\n",
        "        try:\n",
        "            emb1 = np.array(embed_model.get_text_embedding(text1))\n",
        "            emb2 = np.array(embed_model.get_text_embedding(text2))\n",
        "            \n",
        "            # Calculate cosine similarity\n",
        "            similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "            print(f\"'{text1}' vs '{text2}': {similarity:.4f}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating similarity for '{text1}' vs '{text2}': {e}\")\n",
        "\n",
        "def test_context_memory():\n",
        "    \"\"\"Test context memory limits and behavior\"\"\"\n",
        "    print(\"\\n CONTEXT MEMORY TEST\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Clear context\n",
        "    context_handler.clear_history()\n",
        "    \n",
        "    # Add more items than max_history\n",
        "    for i in range(context_handler.max_history + 3):\n",
        "        context_handler.add_to_history(f\"Question {i+1}\", f\"Answer {i+1}\")\n",
        "    \n",
        "    print(f\"Added {context_handler.max_history + 3} items\")\n",
        "    print(f\"Context history length: {len(context_handler.conversation_history)}\")\n",
        "    print(f\"Max history setting: {context_handler.max_history}\")\n",
        "    \n",
        "    if len(context_handler.conversation_history) == context_handler.max_history:\n",
        "        print(\" Context memory limit working correctly\")\n",
        "    else:\n",
        "        print(\" Context memory limit not working as expected\")\n",
        "    \n",
        "    # Show what's in memory\n",
        "    print(\"\\nItems in context memory:\")\n",
        "    for i, item in enumerate(context_handler.conversation_history):\n",
        "        print(f\"  {i+1}. Q: {item['question']}\")\n",
        "\n",
        "def run_quick_test():\n",
        "    \"\"\"Quick test of core functionality\"\"\"\n",
        "    print(\" QUICK SYSTEM TEST\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    try:\n",
        "        # Test basic query\n",
        "        response, _ = ask_question(\"What is Python?\", use_context=False)\n",
        "        if response and len(response) > 20:\n",
        "            print(\" Basic functionality working\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\" Basic functionality failed\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\" Quick test error: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\" Test utility functions defined\")\n",
        "print(\"\\nAvailable test functions:\")\n",
        "print(\"- run_comprehensive_test(): Complete system test\")\n",
        "print(\"- test_embedding_similarity(): Test embedding quality\")\n",
        "print(\"- test_context_memory(): Test context handling limits\")\n",
        "print(\"- run_quick_test(): Quick functionality check\")\n",
        "print(\"\\nRun any of these functions to test the system!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "56fd6041",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " CONTEXT MEMORY TEST\n",
            "========================================\n",
            "Added 8 items\n",
            "Context history length: 5\n",
            "Max history setting: 5\n",
            " Context memory limit working correctly\n",
            "\n",
            "Items in context memory:\n",
            "  1. Q: Question 4\n",
            "  2. Q: Question 5\n",
            "  3. Q: Question 6\n",
            "  4. Q: Question 7\n",
            "  5. Q: Question 8\n"
          ]
        }
      ],
      "source": [
        "test_context_memory()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
