{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d5aad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import youtube_transcript_api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b0f2388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords (run once)\n",
    "# try:\n",
    "#     nltk.data.find('corpus/stopwords')\n",
    "# except LookupError:\n",
    "#     nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d478dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Hugging Face model and tokenizer for embeddings\n",
    "def initialize_embedding_model():\n",
    "    embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "    model = AutoModel.from_pretrained(embedding_model)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5ec41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate embeddings for text\n",
    "def get_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42d390ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocessing - Clean and normalize text\n",
    "def preprocess_text(text, preserve_code=True):\n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove special characters, keep alphanumeric and basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?-]', ' ', text)\n",
    "    \n",
    "    # Replace multiple spaces, newlines, or tabs with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Convert to lowercase (except for code snippets if preserve_code is True)\n",
    "    if not preserve_code:\n",
    "        text = text.lower()\n",
    "    else:\n",
    "        # Preserve code snippets (assuming they are within triple backticks or indentation)\n",
    "        code_blocks = []\n",
    "        def code_replacement(match):\n",
    "            code_blocks.append(match.group(0))\n",
    "            return f\"__CODE_BLOCK_{len(code_blocks)-1}__\"\n",
    "        \n",
    "        text = re.sub(r'```[\\s\\S]*?```', code_replacement, text)\n",
    "        text = re.sub(r'^\\s{4,}.*$', code_replacement, text, flags=re.MULTILINE)\n",
    "        text = text.lower()\n",
    "        # Restore code blocks\n",
    "        for i, code in enumerate(code_blocks):\n",
    "            text = text.replace(f\"__CODE_BLOCK_{i}__\", code)\n",
    "    \n",
    "    # Optionally remove stopwords (disabled by default for programming context)\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # words = text.split()\n",
    "    # text = ' '.join(word for word in words if word.lower() not in stop_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6197825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract text from PDF with structuring\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text_chunks = []\n",
    "            current_section = \"\"\n",
    "            current_text = \"\"\n",
    "            \n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text() or \"\"\n",
    "                lines = text.split('\\n')\n",
    "                for line in lines:\n",
    "                    # Heuristic: Assume lines with all caps or short length are headings\n",
    "                    if line.isupper() or (len(line.strip()) < 50 and line.strip().endswith(':')):\n",
    "                        if current_text:\n",
    "                            text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "                            current_text = \"\"\n",
    "                        current_section = line.strip()\n",
    "                    else:\n",
    "                        current_text += \" \" + line\n",
    "                if current_text:\n",
    "                    text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "                    current_text = \"\"\n",
    "            \n",
    "            return text_chunks  # List of (section, text) tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc24dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract text from website with structuring\n",
    "def extract_text_from_website(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove script, style, and navigation elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "            element.decompose()\n",
    "        \n",
    "        text_chunks = []\n",
    "        current_section = \"\"\n",
    "        current_text = \"\"\n",
    "        \n",
    "        # Extract headings (h1, h2, h3) and their content\n",
    "        for element in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
    "            if element.name in ['h1', 'h2', 'h3']:\n",
    "                if current_text:\n",
    "                    text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "                    current_text = \"\"\n",
    "                current_section = element.get_text(strip=True)\n",
    "            else:\n",
    "                current_text += \" \" + element.get_text(strip=True)\n",
    "        \n",
    "        if current_text:\n",
    "            text_chunks.append((current_section, preprocess_text(current_text)))\n",
    "        \n",
    "        return text_chunks  # List of (section, text) tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from website {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679baaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Extract text from YouTube video transcript with structuring\n",
    "def extract_youtube_transcript(video_url):\n",
    "    try:\n",
    "        video_id = None\n",
    "        if \"youtube.com\" in video_url or \"youtu.be\" in video_url:\n",
    "            match = re.search(r\"(?:v=|youtu\\.be/)([0-9A-Za-z_-]{11})\", video_url)\n",
    "            if match:\n",
    "                video_id = match.group(1)\n",
    "        \n",
    "        if not video_id:\n",
    "            print(f\"Invalid YouTube URL: {video_url}\")\n",
    "            return []\n",
    "            \n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        text_chunks = []\n",
    "        current_text = \"\"\n",
    "        current_timestamp = 0\n",
    "        \n",
    "        # Group transcript by time intervals (e.g., every 60 seconds)\n",
    "        for entry in transcript:\n",
    "            if entry['start'] > current_timestamp + 60:\n",
    "                if current_text:\n",
    "                    text_chunks.append((f\"Segment_{int(current_timestamp)}\", preprocess_text(current_text)))\n",
    "                    current_text = \"\"\n",
    "                current_timestamp = entry['start']\n",
    "            current_text += \" \" + entry['text']\n",
    "        \n",
    "        if current_text:\n",
    "            text_chunks.append((f\"Segment_{int(current_timestamp)}\", preprocess_text(current_text)))\n",
    "        \n",
    "        return text_chunks  # List of (segment, text) tuples\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting transcript from YouTube video {video_url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d73ab253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Cache preprocessed text to disk\n",
    "def cache_text(source, text_chunks, cache_dir=\"../cache\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    source_name = re.sub(r'[^\\w\\-_\\.]', '_', source)\n",
    "    cache_path = os.path.join(cache_dir, f\"{source_name}.txt\")\n",
    "    \n",
    "    try:\n",
    "        with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "            for section, text in text_chunks:\n",
    "                f.write(f\"--- {section} ---\\n{text}\\n\\n\")\n",
    "        print(f\"Cached text for {source} to {cache_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error caching text for {source}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90949e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Check cache for preprocessed text\n",
    "def load_from_cache(source, cache_dir=\"../cache\"):\n",
    "    source_name = re.sub(r'[^\\w\\-_\\.]', '_', source)\n",
    "    cache_path = os.path.join(cache_dir, f\"{source_name}.txt\")\n",
    "    \n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                chunks = []\n",
    "                current_section = \"\"\n",
    "                current_text = \"\"\n",
    "                for line in content.splitlines():\n",
    "                    if line.startswith(\"--- \") and line.endswith(\" ---\"):\n",
    "                        if current_text:\n",
    "                            chunks.append((current_section, current_text.strip()))\n",
    "                            current_text = \"\"\n",
    "                        current_section = line[4:-4].strip()\n",
    "                    else:\n",
    "                        current_text += \" \" + line\n",
    "                if current_text:\n",
    "                    chunks.append((current_section, current_text.strip()))\n",
    "                return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cache for {source}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adb05a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Initialize embedding model and generate sample embeddings for review\n",
    "def generate_sample_embeddings(text_chunks, sources, num_samples=5):\n",
    "    \"\"\"\n",
    "    Initialize embedding model and generate sample embeddings for review.\n",
    "    Does not store embeddings, only displays them for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        text_chunks: List of text chunks from different sources\n",
    "        sources: List of source names corresponding to text_chunks\n",
    "        num_samples: Number of sample embeddings to generate and display\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the embedding model\n",
    "    print(\"Initializing embedding model...\")\n",
    "    tokenizer, model = initialize_embedding_model()\n",
    "    print(\"Embedding model initialized successfully!\\n\")\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    for source, chunks in zip(sources, text_chunks):\n",
    "        if not chunks or sample_count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        print(f\"Processing source: {source}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (section, text) in enumerate(chunks):\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            # Split text into smaller chunks (~500 words) for embedding\n",
    "            words = text.split()\n",
    "            chunk_size = 500\n",
    "            sub_chunks = [\" \".join(words[j:j + chunk_size]) for j in range(0, len(words), chunk_size)]\n",
    "            \n",
    "            for j, sub_chunk in enumerate(sub_chunks):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                # Generate embedding for the sub-chunk\n",
    "                print(f\"\\nSample {sample_count + 1}:\")\n",
    "                print(f\"Source: {source}\")\n",
    "                print(f\"Section: {section}\")\n",
    "                print(f\"Chunk ID: {i}_{j}\")\n",
    "                print(f\"Text preview (first 100 chars): {sub_chunk[:100]}...\")\n",
    "                \n",
    "                # Get the embedding\n",
    "                embedding = get_embeddings(sub_chunk, tokenizer, model)\n",
    "                \n",
    "                # Display embedding information\n",
    "                print(f\"Embedding shape: {embedding.shape}\")\n",
    "                print(f\"Embedding type: {type(embedding)}\")\n",
    "                print(f\"First 10 dimensions: {embedding[:10].tolist()}\")\n",
    "                # print(f\"Embedding norm: {float(embedding.norm()):.4f}\")\n",
    "                print(f\"Min value: {float(embedding.min()):.4f}\")\n",
    "                print(f\"Max value: {float(embedding.max()):.4f}\")\n",
    "                print(f\"Mean value: {float(embedding.mean()):.4f}\")\n",
    "                print(\"=\" * 60)\n",
    "                \n",
    "                sample_count += 1\n",
    "    \n",
    "    print(f\"\\nGenerated and displayed {sample_count} sample embeddings for review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c006bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to process all sources\n",
    "def process_knowledge_base(pdf_paths, website_urls, youtube_urls, cache_dir=\"..\\cache\"):\n",
    "    texts = []\n",
    "    sources = []\n",
    "    \n",
    "    # Process PDFs\n",
    "    for pdf_path in pdf_paths:\n",
    "        cached = load_from_cache(pdf_path, cache_dir)\n",
    "        if cached:\n",
    "            texts.append(cached)\n",
    "            sources.append(pdf_path)\n",
    "        else:\n",
    "            chunks = extract_text_from_pdf(pdf_path)\n",
    "            if chunks:\n",
    "                cache_text(pdf_path, chunks, cache_dir)\n",
    "                texts.append(chunks)\n",
    "                sources.append(pdf_path)\n",
    "    \n",
    "    # Process websites\n",
    "    for url in website_urls:\n",
    "        cached = load_from_cache(url, cache_dir)\n",
    "        if cached:\n",
    "            texts.append(cached)\n",
    "            sources.append(url)\n",
    "        else:\n",
    "            chunks = extract_text_from_website(url)\n",
    "            if chunks:\n",
    "                cache_text(url, chunks, cache_dir)\n",
    "                texts.append(chunks)\n",
    "                sources.append(url)\n",
    "    \n",
    "    # Process YouTube videos\n",
    "    for video_url in youtube_urls:\n",
    "        cached = load_from_cache(video_url, cache_dir)\n",
    "        if cached:\n",
    "            texts.append(cached)\n",
    "            sources.append(video_url)\n",
    "        else:\n",
    "            chunks = extract_youtube_transcript(video_url)\n",
    "            if chunks:\n",
    "                cache_text(video_url, chunks, cache_dir)\n",
    "                texts.append(chunks)\n",
    "                sources.append(video_url)\n",
    "    \n",
    "    # generate_sample_embeddings for review\n",
    "    generate_sample_embeddings(texts, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b394407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding model...\n",
      "Embedding model initialized successfully!\n",
      "\n",
      "Processing source: ../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 1:\n",
      "Source: ../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\n",
      "Section: FOURTH EDITION\n",
      "Chunk ID: 0_0\n",
      "Text preview (first 100 chars): tony gaddisstarting out with python...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [-0.19292797148227692, 0.13389192521572113, -0.12019527703523636, -0.11675722151994705, -0.1036008670926094, -0.4054447114467621, 0.4728517532348633, -0.01164678018540144, -0.5176032781600952, -0.4057198762893677]\n",
      "Min value: -0.5997\n",
      "Max value: 0.6762\n",
      "Mean value: -0.0023\n",
      "============================================================\n",
      "\n",
      "Sample 2:\n",
      "Source: ../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\n",
      "Section: FOURTH EDITION\n",
      "Chunk ID: 1_0\n",
      "Text preview (first 100 chars): digital resources for students your new textbook provides 12-month access to digital resources that ...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [-0.19033236801624298, -0.057878557592630386, -0.22134360671043396, -0.08538810908794403, -0.12244727462530136, -0.17459242045879364, 0.017202768474817276, 0.14634151756763458, -0.12334497272968292, 0.04886263981461525]\n",
      "Min value: -0.4155\n",
      "Max value: 0.3796\n",
      "Mean value: 0.0013\n",
      "============================================================\n",
      "\n",
      "Sample 3:\n",
      "Source: ../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\n",
      "Section: IMPORTANT:\n",
      "Chunk ID: 2_0\n",
      "Text preview (first 100 chars): this prepaid subscription does not include access to myprogramminglab, which is available at www.myp...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [-0.16537192463874817, -0.20834697782993317, -0.06808752566576004, -0.16159579157829285, -0.06373018026351929, 0.11176694929599762, 0.15346167981624603, 0.0006214767345227301, -0.0608702078461647, -0.019361380487680435]\n",
      "Min value: -0.3615\n",
      "Max value: 0.4134\n",
      "Mean value: -0.0003\n",
      "============================================================\n",
      "\n",
      "Sample 4:\n",
      "Source: ../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\n",
      "Section: IMPORTANT:\n",
      "Chunk ID: 3_0\n",
      "Text preview (first 100 chars): this page intentionally left blank...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [0.08669989556074142, 0.37735074758529663, 0.29091671109199524, 0.19894710183143616, 0.3349900245666504, -0.24820612370967865, -0.4171147644519806, -0.41379743814468384, 0.27598103880882263, 0.04877414181828499]\n",
      "Min value: -0.8100\n",
      "Max value: 1.0976\n",
      "Mean value: 0.0019\n",
      "============================================================\n",
      "\n",
      "Sample 5:\n",
      "Source: ../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\n",
      "Section: IMPORTANT:\n",
      "Chunk ID: 4_0\n",
      "Text preview (first 100 chars): python p ythonstarting out with fourth edition global edition...\n",
      "Embedding shape: (384,)\n",
      "Embedding type: <class 'numpy.ndarray'>\n",
      "First 10 dimensions: [0.18903130292892456, -0.07819569855928421, 0.2151629626750946, -0.16583581268787384, 0.10748692601919174, -0.19848376512527466, -0.37171971797943115, 0.038145530968904495, -0.3764508366584778, -0.25392428040504456]\n",
      "Min value: -0.7702\n",
      "Max value: 0.7080\n",
      "Mean value: -0.0011\n",
      "============================================================\n",
      "\n",
      "Generated and displayed 5 sample embeddings for review.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_paths = [\n",
    "        \"../data/raw_data/Starting Out with Python, Global Edition, 4th Edition.pdf\"\n",
    "    ]\n",
    "    website_urls = [\n",
    "        \"https://www.geeksforgeeks.org/how-to-learn-python-from-scratch/\"\n",
    "    ]\n",
    "    youtube_urls = [\n",
    "        \"https://www.youtube.com/watch?v=8124kv-632k\"\n",
    "    ]\n",
    "    \n",
    "    process_knowledge_base(pdf_paths, website_urls, youtube_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
